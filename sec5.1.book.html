<!doctype html>
<html lang="en-US" xml:lang="en-US">
	<head>
		<title>5.1 Homogeneous Linear Equations</title>
		<meta charset="utf-8" />
		<meta content="TeX4ht (https://tug.org/tex4ht/)" name="generator" />
		<meta content="width=device-width,initial-scale=1" name="viewport" />
		<link rel="stylesheet" type="text/css" href="combined.css" />
		<script type="text/javascript" src="niceties.js"></script>
		<meta content="Trench_DiffEQ_Book.tex" name="src" />
		<script>
			window.MathJax = {
				tex: {
					packages: { "[+]": ["tagformat"] },
					tags: "ams",
					tagformat: { tag: (tag) => "(5.1." + tag + ")" },
				},
			};
		</script>
		<script
			async="async"
			id="MathJax-script"
			src="mathjax-3-es5-tex-chtml-full.js"
			type="text/javascript"
		></script>
		<style type="text/css">
			:root {
				--section-number: "5.1";
			}
		</style>
	</head>
	<body>
		<div id="3ac7df10-b122-4149-aa5b-3a959ab6b43b" class="section-title">
			5.1 Homogeneous Linear Equations
		</div>

	

		<!--Homogeneous Linear Equations-->
		<details id="54f23eb8-7365-45fc-ad4f-e14b44acf04d">
			<summary>Homogeneous Linear Equations</summary>
		
			<div id="d645654c-c987-442a-b5eb-abf6685bc698">
				A second order differential equation is said to be <span class="vocab">linear</span> if it can be written as 
				
				<a id="x1-2r1"></a>
				\begin {equation} \label {eq:5.1.1}
					\frac{d^2y}{dx^2}+p(x)\frac{dy}{dx}+q(x)y=f(x)
				\end {equation}
				
				We call the function \(f(x)\) on the right a <span class="vocab">forcing function</span>,
				since in physical applications it’s often related to a force acting on some system modeled by the differential equation. We say that <a href="#x1-2r1">(5.1.1)</a> is <span class="vocab">homogeneous</span> if the forcing function \(f(x)\equiv 0\) or <span class="vocab">nonhomogeneous</span> if \(f(x)\not \equiv 0\). Since these definitions are like the corresponding definitions in Section&nbsp;2.1 for the linear first order equation 
				
				<a id="x1-3r2"></a>
				\begin{equation} \label {eq:5.1.2} 
					\frac{dy}{dx}+p(x)y=f(x)
				\end {equation}
				
				it’s natural to expect similarities between methods of solving <a href="#x1-2r1">(5.1.1)</a> and <a href="#x1-3r2">(5.1.2)</a>. However, solving <a href="#x1-2r1">(5.1.1)</a> is more difficult than solving <a href="#x1-3r2">(5.1.2)</a>. For example, while Theorem&nbsp; <span class="ptmb8t-">2.1.1</span> and Theorem&nbsp; <span class="ptmb8t-">2.1.2</span> give general solutions of all first order linear equations <a href="#x1-3r2">(5.1.2)</a>, <em>there are no formulas for the general solution of <a href="#x1-2r1">(5.1.1)</a></em>. Therefore we must be content to solve only linear second order equations of special forms.
			</div>
			<div class="indent" id="43af8b13-63ec-4e0d-8870-9e2002119e3b">
				In Section&nbsp;2.1 we used <span class="vocab">variation of parameters</span> by considering the homogeneous equation \({dy}/{dx}+p(x)y=0\) first, and then used a simple nontrivial solution to find the general solution of the nonhomogeneous equation \({dy}/{dx}+p(x)y=f(x)\). Although the progression from the homogeneous to the nonhomogeneous case isn’t that simple for second order equation, it’s still necessary to solve the homogeneous equation 
				
				<a id="x1-4r3"></a>
				\begin {equation} \label {eq:5.1.3} 
					\frac{d^2y}{dx^2} +p(x)\frac{dy}{dx}+q(x)y=0 
				\end {equation}
				
				first in order to solve the nonhomogeneous equation <a href="#x1-2r1">(5.1.1)</a>. This section is devoted to verifying solutions to linear homogeneous second order equations <a href="#x1-4r3">(5.1.3)</a> as well as some other important techniques such as using the Wronskian to determine whether or not two solutions are linearly independent.
			</div>
	


			<div class="theorem" id="198cbe42-7032-4493-b0d4-8482ad90bf09">
				<div>
					<span class="head">Theorem&nbsp;5.1.1</span>

					<a id="x1-51"></a>
					Suppose \(p(x)\) and \(q(x)\) are continuous on an open interval \((a,b),\) let \(x_0\) be any point in \((a,b),\) and let \(k_0\) and \(k_1\) be arbitrary real numbers\(.\) Then the initial value problem has a unique solution on \((a,b)\)
					
					\[
						\frac{d^2y}{dx^2}+p(x)\frac{dy}{dx}+q(x)y=0
						\quad \ y(x_0)=k_0\
						\quad \left.\frac{dy}{dx}\right|_{x_0}=k_1 
					\] 
					
					
				</div>
			</div>

			<div class="indent" id="0ef53e7f-77f6-4789-91ff-c2df4f6e9198">
				Since \(y \equiv 0\) is obviously a solution of <a href="#x1-4r3">(5.1.3)</a> we call it the <span class="vocab">trivial</span> solution. Any other solution is <span class="vocab">nontrivial</span>. Under the assumptions of Theorem&nbsp;<a href="#x1-51">5.1.1</a>, the only solution of the initial value problem (IVP) on \((a,b)\) is the trivial solution (Exercise&nbsp; <span class="ptmb8t-">24</span>).
				
				\[ 
					\frac{d^2y}{dx^2}+p(x)\frac{dy}{dx}+q(x)y=0
					\quad y(x_0)=0
					\quad \left.\frac{dy}{dx}\right|_{x_0}=0
				\] 
			</div>
		</details>

		<!--Verifying Solutions-->
		<details id="54f23eb8-7365-45fc-ad4f-e14b44acf04drew">	
			<summary>Verifying Solutions</summary>
			
			<div class="indent" id="ba67d92f-5f10-4f37-8a5f-f2c2e5a91673">
				The next three examples illustrate concepts that we’ll develop later in this section. You shouldn’t be concerned with how to <em>find</em> the given solutions of the equations in these examples. This will be explained in later sections.
			</div>

			<!--Example 1-->
			<div class="example" id="x1-61" style="--ex-number: 1">
				<div>
					Consider the linear second order equation:
					
					<a id="x1-7r4"></a>
					\begin {equation} \label{eq:5.1.4} 
						\frac{d^2y}{dx^2}-y=0 
					\end {equation}
				</div>

				<ol>
					<li>
						Determine where a solutions exists for IVPs of <a href="#x1-7r4">(5.1.4)</a> 	    
					</li>	
					<li>						
						Verify that \(y_1=e^x\) and \(y_2=e^{-x}\) are solutions of <a href="#x1-7r4">(5.1.4)</a> on \((-\infty ,\infty )\)
					</li>
					<li>
						Verify that \(y=c_1e^x+c_2e^{-x}\) is a solution of <a href="#x1-7r4">(5.1.4)</a> on \((-\infty ,\infty )\)
					</li>
					<li>
						Solve the IVP 

						<a id="x1-11r5"></a>
						\begin {equation} \label {eq:5.1.5}
							\frac{d^2y}{dx^2}-y=0
							\quad y(0)=1
							\quad \left.\frac{dy}{dx}\right|_{0}=3
						\end {equation}
					</li>
				</ol>
					


				<details class="solution" id="d3cafea5-d9fe-4743-bd86-24a64d90c53f">
					<summary>Solution</summary>
					<ol>
						<li>
							<div>
								Using linear standard form <a href="#x1-2r1">(5.1.1)</a> we can identify  \(p(x) = 0\) and \(q(x) = -1\) which are both constant functions and continuous everywhere \((-\infty ,\infty )\) therefore Theorem&nbsp;<a href="#x1-51">5.1.1</a> indicates that every IVP for <a href="#x1-7r4">(5.1.4)</a> has a unique solution on \((-\infty ,\infty )\)
							</div>
							<div>
							</div>	
							<div>
							</div>	
							<div>
							</div>	
							<div>
							</div>	
						</li>	
						<li>
							<div>
								Start by checking for discontinuities in \(y_1\) and \(y_2.\) Since they are exponential functions they are defined everywhere so their domains are \((-\infty ,\infty )\) 
							</div>	
							<div>
								Now calculate the second derivatives of both \(y_1\) and \(y_2\) and then substitute them into <a href="#x1-7r4">(5.1.4)</a> to verify. 
							</div>
							<div class="reading-exercise">
								Differentiate \( y_1 = e^x \) twice
							</div> 	
							<div>
								We’ll start with \(y_1\) since it is simpler. Differentiating:

								\begin{align*}
									y_1 = & e^x \Longrightarrow
									\frac{dy_1}{dx} = e^x \Longrightarrow
									\frac{d^2y_1}{dx^2} = e^x \\		
								\end{align*} 
								
								Substituting into <a href="#x1-7r4">(5.1.4)</a>
								
								\begin{align*}
									\frac{d^2 y}{dx^2} - y = & 0 \\	
									\underset{e^x}{\underbrace{\frac{d^2y_1}{dx^2}}}  - \underset{e^x}{\underbrace{y_1}} \overset{?}{=} & 0 \\
									e^x - e^x = & 0 \quad \checkmark
								\end{align*} 								
							</div>	
							<div class="reading-exercise">
								Differentiate \( y_2 = e^{-x} \) twice
							</div> 	
							<div>
								Now we'll verify \(y_2\) is a solution. Differentiating:
 
								\begin{align*}
									y_2 = & e^{-x} \\
									\frac{dy_2}{dx} = & e^{-x}\frac{d}{dx}(-x) = -e^{-x} \\
									\frac{d^2y_2}{dx^2} = & \frac{d}{dx}(- e^{-x}) = -e^{-x} \frac{d}{dx}(-x) = e^{-x} \\		
								\end{align*} 
								
								Substituting into <a href="#x1-7r4">(5.1.4)</a>
								 
								\begin{align*}
									\frac{d^2 y}{dx^2} - y = & 0 \\	
									\underset{e^{-x}}{\underbrace{\frac{d^2y_2}{dx^2}}}  - \underset{e^{-x}}{\underbrace{y_2}} \overset{?}{=} & 0 \\
									e^{-x} - e^{-x} = & 0 \quad \checkmark
								\end{align*} 								
							</div>	
							<div>
								Therefore \(y_1=e^x\) and \(y_2=e^{-x}\) are solutions of <a href="#x1-7r4">(5.1.4)</a> on \((-\infty ,\infty )\)
							</div>	
							<div>
							</div>	
						</li>
						<li>
							<div>
								Start by checking for discontinuities. Since <a href="#x1-12r6">(5.1.6)</a> is just the sum of exponential functions it is defined everywhere so its domain is \((-\infty ,\infty )\) 
							</div>	
							<div>
								<a id="x1-12r6"></a>
								\begin {equation} \label {eq:5.1.6} 
									y=c_1e^x+c_2e^{-x} 
								\end {equation}

								Now find the second derivative <a href="#x1-12r6">(5.1.6)</a> and substitute into <a href="#x1-7r4">(5.1.4)</a> to verify
							</div>
							<div class="reading-exercise">
								Differentiate <a href="#x1-12r6">(5.1.6)</a> twice using the chain rule
							</div> 
							<div>
								<a id="x1-13r7"></a>
								\begin{align}
									\frac{dy}{dx} 
									= & \frac{d}{dx} \left( c_1e^x+c_2e^{-x} \right)
									= c_1 e^x - c_2e^{-x}  \nonumber \\  \nonumber \\
								\frac{d^2y}{dx^2} 
									= & \frac{d}{dx} \left( c_1 e^x - c_2e^{-x} \right)	
									= c_1 e^x +  c_2e^{-x}  
								\end{align} 
								
								Substituting <a href="#x1-12r6">(5.1.6)</a> and <a href="#x1-13r7">(5.1.7)</a> into <a href="#x1-7r4">(5.1.4)</a>
							
							</div>
							<div>
								\begin{align*}
								\begin{array}{rrl}
								  \displaystyle \frac{d^2y}{dx^2}   = &amp; c_1 e^x   &amp; + c_2 e^{-x} \\
								  + \quad \quad \quad \quad -y  	= &amp; -c_1 e^x &amp; -c_2 e^{-x}   \\
								  \hline \\
								  \displaystyle \frac{d^2y}{dx^2} 
									- y                           	= &amp; \overset{1-1}{\overbrace{0}}c_1 e^x 
																	  &amp;  + \overset{1-1}{\overbrace{0}}c_2 e^{-x} \\ \\
								  \displaystyle \frac{d^2y}{dx^2} 
								  	- y                            \overset{\checkmark}{=} &amp; 0 \quad \quad \quad &amp; \\                
								\end{array}
							  \end{align*}      
					
							</div>	
							<div>
								Therefore \(y=c_1e^x+c_2e^{-x}\) is a solution of <a href="#x1-7r4">(5.1.4)</a> on \((-\infty ,\infty )\)
							</div>	
							<div>
							</div>	
							<div>
							</div>	
						</li>
						<li>
							<div>
								In Part 3 we already verified \(y=c_1e^x+c_2e^{-x}\) is a solution of <a href="#x1-7r4">(5.1.4)</a> and we computed its derivative so we have:

								\begin{align*}
									y= & c_1e^x+c_2e^{-x} \\ \nonumber \\
									\frac{dy}{dx} = & c_1 e^x - c_2e^{-x}  \nonumber \\
								\end{align*} 
							</div>
							<div>
								To solve the IVP we simply apply the initial conditions 
								\( y(0)=1 \) and \( \displaystyle \left.\frac{dy}{dx}\right|_{0}=3 \)
							</div>	
							<div>
								\begin{align*}
									y(x) = & c_1e^x+c_2e^{-x} \\ 
									y(0) = & c_1e^0+c_2e^{-0} \\  
									1 = & c_1 + c_2 \\ \\	
									\frac{dy}{dx} = & c_1 e^x - c_2e^{-x}  \\
									\left.\frac{dy}{dx}\right|_{0} = & c_1 e^0 - c_2e^{-0}  \\	
									3= & c_1 - c_2  \\	
								\end{align*} 								
							</div>	
							<div>
								And we obtain a system of equations which we can solve by adding them together to find \(c_2\) then substituting that back to find \(c_1\)
								\begin{align*}
								\left\{
									\begin{array}{r l}
										c_1 + c_2 = & 1 \\
										c_1 - c_2 = & 3 \\
									\end{array}
								\right.	
								\longrightarrow
								\left\{
									\begin{array}{r l}
										c_1 + c_2 = & 1 \\
										2c_1 = & 4 \\
									\end{array}
								\right.	
								\longrightarrow
								\left\{
									\begin{array}{r l}
										2 + c_2 = & 1 \\
										c_1 = & 2 \\
									\end{array}
								\right.		
								\longrightarrow
								\left\{
									\begin{array}{r l}
										c_2 = & -1 \\
										c_1 = & 2 \\
									\end{array}
								\right.									 		
							\end{align*}								
							</div>	
							<div>
								Substituting \(c_1 = 2\) and \(c_2 = -1\) into <a href="#x1-12r6">(5.1.6)</a> we obtain our unique solution to the IVP <a href="#x1-11r5">(5.1.5)</a> on \((-\infty ,\infty )\)

								\[
									y= 2e^x - e^{-x}
								\]
							</div>	
							<div class="reading-exercise">
								Verify \( y= 2e^x - e^{-x} \) is a solution to <a href="#x1-7r4">(5.1.4)</a> 
							</div> 
						</li>
					</ol>
				</details>
			</div>

			<!--Example 2-->
			<div class="example" id="x1-142" style="--ex-number: 2">
				<div>
					<a id="x1-"></a>
					Let \(\omega \) be a positive constant. Consider the linear second order equation:  
					
					<a id="x1-15r8"></a>
					\begin {equation} \label {eq:5.1.8} 
						\frac{d^2y}{dx^2}+\omega ^2y=0 
					\end{equation}
				</div>
				<ol>
					<li>
						Determine where a solutions exists for IVPs of <a href="#x1-15r8">(5.1.8)</a> 	    
					</li>
					<li>
						Verify that \(y_1=\cos \omega x\) and \(y_2=\sin \omega x\) are solutions of <a href="#x1-15r8">(5.1.8)</a> on \((-\infty ,\infty )\)
					</li>
					<li>
						Verify that \(y=c_1\cos \omega x+c_2\sin \omega x\) is a solution of <a href="#x1-15r8">(5.1.8)</a> on \((-\infty ,\infty )\)
					</li>
					<li>
						<a id="x1-19r9"></a>
						Solve the IVP 
						
						\begin {equation} \label {eq:5.1.9}
							\frac{d^2y}{dx^2}+\omega ^2y=0
							\quad y(0)=1
							\quad \left.\frac{dy}{dx}\right|_{0}=3
						\end {equation}
					</li>
				</ol>
				<details class="solution" id="857aa373-5a74-4453-bd1c-ec8c4b2b8989">
					<summary>Solution</summary>

					<ol>
						<li>
							<div>
								Using linear standard form <a href="#x1-2r1">(5.1.1)</a> we can identify  \(p(x) = 0\) and \(q(x) = \omega ^2\) which are both constant functions and continuous everywhere \((-\infty ,\infty )\) therefore Theorem&nbsp;<a href="#x1-51">5.1.1</a> indicates that every IVP for <a href="#x1-15r8">(5.1.8)</a> has a unique solution on \((-\infty ,\infty )\)
							</div>
						</li>	
						<li>
							<div>
								Start by checking for discontinuities in \(y_1\) and \(y_2.\) Since they are sines and cosines they are defined everywhere so their domains are \((-\infty ,\infty )\) 
							</div>	
							<div>
								Now calculate the second derivatives of both \(y_1\) and \(y_2\) and then substitute them into <a href="#x1-15r8">(5.1.8)</a> to verify. 
							</div>
							<div class="reading-exercise">
								Differentiate \( y_1 = \cos \omega x \) twice using chain rule
							</div> 	
							<div>
								We’ll start with \(y_1\). Differentiating:

								\begin{align*}
									y_1 = & \cos \omega x \\ 
									\Longrightarrow
									\frac{dy_1}{dx} = & -\omega \sin \omega x \\ 
									\Longrightarrow
									\frac{d^2y_1}{dx^2} = & -\omega^2 \cos \omega x \\		
								\end{align*} 
								
								Substituting into <a href="#x1-15r8">(5.1.8)</a>
								
								\begin{align*}
									\frac{d^2 y}{dx^2} + \omega^2 y = & 0 \\	
									\underset{-\omega^2 \cos \omega x}{\underbrace{\frac{d^2y_1}{dx^2}}}  
										+ \omega^2 \underset{\cos \omega x}{\underbrace{y_1}} \overset{?}{=} & 0 \\
										-\omega^2 \cos \omega x + \omega^2 \cos \omega x = & 0 \quad \checkmark
								\end{align*} 								
							</div>	
							<div class="reading-exercise">
								Differentiate \( y_2 = \sin \omega x \) twice
							</div> 	
							<div>
								Now we'll verify \(y_2\) is a solution. Differentiating:
 
								\begin{align*}
									y_2 = & \sin \omega x \\
									\Longrightarrow
									\frac{dy_2}{dx} = & \omega \cos \omega x \\
									\Longrightarrow
									\frac{d^2y_2}{dx^2} = &  -\omega^2 \sin \omega x \\		
								\end{align*} 
								
								Substituting into <a href="#x1-15r8">(5.1.8)</a>
								 
								\begin{align*}
									\frac{d^2 y}{dx^2} + \omega^2 y = & 0 \\	
									\underset{-\omega^2 \sin \omega x}{\underbrace{\frac{d^2y_2}{dx^2}}}  
										+ \omega^2 \underset{\sin \omega x}{\underbrace{y_2}} \overset{?}{=} & 0 \\
										-\omega^2 \sin \omega x + \omega^2 \sin \omega x = & 0 \quad \checkmark
								\end{align*} 								
							</div>	
							<div>
								Therefore \(y_1 = \cos \omega x \) and \(y_2 = \sin \omega x\) are solutions of <a href="#x1-15r8">(5.1.8)</a> on \((-\infty ,\infty )\)
							</div>	
						</li>
						<li>
							<div>
								Start by checking for discontinuities. Since <a href="#x1-20r10">(5.1.10)</a> is just the sum of sine and cosine it is defined everywhere so its domain is \((-\infty ,\infty )\) 
							</div>	
							<div>
								<a id="x1-20r10"></a>
								\begin {equation} \label {eq:5.1.10} 
									y = c_1 \cos \omega x + c_2 \sin \omega x 
								\end {equation}

								Now find the second derivative <a href="#x1-20r10">(5.1.10)</a> and substitute into <a href="#x1-15r8">(5.1.8)</a> to verify
							</div>
							<div class="reading-exercise">
								Differentiate <a href="#x1-20r10">(5.1.10)</a> twice using the chain rule
							</div> 
							<div>
								<a id="x1-21r11"></a>
								\begin{align} \label {eq:5.1.11}
									\frac{dy}{dx} 
									= & \frac{d}{dx} \left( c_1 \cos \omega x + c_2 \sin \omega x \right) \nonumber  \\
									= & - \omega c_1  \sin \omega x + c_2 \omega \cos \omega x  \nonumber \\  \nonumber \\
								\frac{d^2y}{dx^2} 
									= & \frac{d}{dx} \Big( - \omega c_1  \sin \omega x + c_2 \omega \cos \omega x \Big)	 \nonumber  \\
									= & - \omega^2 c_1  \cos \omega x - \omega^2 c_2 \sin \omega 
								\end{align} 
								
								Substituting <a href="#x1-20r10">(5.1.10)</a> and <a href="#x1-21r11">(5.1.11)</a> into <a href="#x1-15r8">(5.1.8)</a>
							</div>
							<div>
								\begin{align*}
								\begin{array}{rrl}
								  \displaystyle \frac{d^2y}{dx^2}   = &amp; - \omega^2 c_1  \cos \omega x   &amp; - \omega^2 c_2 \sin \omega x \\
								  + \quad\quad\quad\quad\omega^2 y  = &amp; + \omega^2 c_1 \cos \omega x &amp; + \omega^2 c_2 \sin \omega x   \\
								  \hline \\
								  \displaystyle \frac{d^2y}{dx^2} 
									+ \omega^2 y                    = &amp; \overset{-1+1}{\overbrace{0}}\omega^2 c_1  \cos \omega x
																	  &amp;  + \overset{-1+1}{\overbrace{0}}\omega^2 c_2 e^{-x} \sin \omega x \\ \\
								  \displaystyle \frac{d^2y}{dx^2} 
								  	+ \omega^2 y                    \overset{\checkmark}{=} &amp; 0 \quad \quad \quad \quad \quad \quad &amp; \\                
								\end{array}
							  \end{align*}      
							</div>	
							<div>
								Therefore \( y = c_1 \cos \omega x + c_2 \sin \omega x \) is a solution of <a href="#x1-15r8">(5.1.8)</a> on \((-\infty ,\infty )\)
							</div>	
						</li>
						<li>
							<div>
								In Part 3 we already verified \( y = c_1 \cos \omega x + c_2 \sin \omega x \) is a solution of <a href="#x1-15r8">(5.1.8)</a> and we computed its derivative so we have:

								\begin{align*}
									y = & c_1 \cos \omega x + c_2 \sin \omega x \\ \nonumber \\
									\frac{dy}{dx} = & - \omega c_1 \sin \omega x + \omega c_2 \cos \omega x  \nonumber \\
								\end{align*} 
							</div>
							<div>
								To solve the IVP we simply apply the initial conditions 
								\( y(0)=1 \) and \( \displaystyle \left.\frac{dy}{dx}\right|_{0}=3 \)
							</div>	
							<div>
								\begin{align*}
									y(x) = & c_1 \cos \omega x + c_2 \sin \omega x \\ 
									y(0) = & c_1 \cos (\omega \cdot 0) + c_2 \sin (\omega \cdot 0) \\  
									1 = & c_1 \cdot 1 + c_2 \cdot 0 
									\Longrightarrow c_1 = 1 \\ \\	
									\frac{dy}{dx} = & - \omega c_1 \sin \omega x + \omega c_2 \cos \omega x   \\
									\left.\frac{dy}{dx}\right|_{x=0} = & - \omega c_1 \sin (\omega \cdot 0 ) + \omega c_2 \cos ( \omega \cdot 0 )   \\
									3 = & 	- \omega c_1 \cdot 0  + \omega c_2 \cdot 1 \\
									3= & \omega c_2 
									\Longrightarrow 
									c_2 = \frac{3}{\omega}
								\end{align*} 								
							</div>	

							<div>
								Substituting \(c_1 = 1\) and \(c_2 = \frac{3}{\omega}\) into <a href="#x1-20r10">(5.1.10)</a> we obtain our unique solution to the IVP <a href="#x1-19r9">(5.1.9)</a> on \((-\infty ,\infty )\)

								\[
									y= \cos \omega x + \frac{3}{\omega} \sin \omega x
								\]
							</div>	
							<div class="reading-exercise">
								Verify \( y= \cos \omega x + \frac{3}{\omega} \sin \omega x \) is a solution to <a href="#x1-15r8">(5.1.8)</a> 
							</div> 
						</li>
					</ol>

				</details>
			</div>

			<br>
			<br>

			<!--Blurb after Example 2 / Blurb before Example 3-->
			<div class="indent" id="994bd946-38c4-47d2-9892-c54f712d2e08">
				Theorem&nbsp;<a href="#x1-51">5.1.1</a> implies that if \(k_0\) and \(k_1\) are arbitrary real numbers then the IVP 
				
				<a id="x1-22r12"></a>
				\begin {equation} \label {eq:5.1.12}
					P_0(x)\frac{d^2y}{dx^2}+P_1(x)\frac{dy}{dx}+P_2(x)y=0
					\quad y(x_0)=k_0
					\quad \left.\frac{dy}{dx}\right|_{x_0}=k_1 
				\end{equation}
				
				has a unique solution on an interval \((a,b)\) that contains \(x_0\), provided that \(P_0(x)\), \(P_1(x)\), and \(P_2(x)\) are continuous and \(P_0(x)\) has no zeros on \((a,b)\). To see this, we rewrite the differential equation in <a href="#x1-22r12">(5.1.12)</a> as 
				
				\[ 
					\frac{d^2y}{dx^2}+{P_1(x)\over P_0(x)}\frac{dy}{dx}+{P_2(x)\over P_0(x)}y=0 
				\] 
				
				and apply Theorem&nbsp;<a href="#x1-51">5.1.1</a> with \(p(x) = \displaystyle  \frac{P_1(x)}{P_0(x)}\) and \(q(x) = \displaystyle \frac{P_2(x)}{P_0(x)}\)
			</div>


			<!--Example 3-->
			<div class="example" id="x1-233" style="--ex-number: 3">
				<div>
					Consider the linear second order equation:
					<a id="x1-24r13"></a>
					\begin {equation} \label {eq:5.1.13} 
						x^2 \frac{d^2y}{dx^2} + x \frac{dy}{dx} - 4 y = 0 
					\end {equation}
				</div>

				<ol>
					<li>
						Determine where a solutions exists for IVPs of <a href="#x1-24r13">(5.1.13)</a> 	    
					</li>
					<li>
						Verify that \(y_1=x^2\) is a solution of <a href="#x1-24r13">(5.1.13)</a> on \((-\infty ,\infty )\) and \(y_2=1/x^2\) is a solution of <a href="#x1-24r13">(5.1.13)</a> on \((-\infty ,0)\) and \((0,\infty )\)
					</li>
					<li>
						Verify that if \(c_1\) and \(c_2\) are any constants then
						\(y = c_1 x^2 + c_2/x^2 \) is a solution of <a href="#x1-24r13">(5.1.13)</a> on \((-\infty ,0)\) and \((0,\infty )\)
					</li>
					<li>
						Solve the IVP 

						<a id="x1-28r14"></a>
						\begin {equation} \label {eq:5.1.14}
							x^2\frac{d^2y}{dx^2}+x\frac{dy}{dx}-4y=0
							\quad y(1)=2
							\quad \left.\frac{dy}{dx}\right|_{1}=0
						\end {equation}
					</li>
					<li>
						Solve the IVP 
						
						<a id="x1-30r15"></a>
						\begin {equation} \label {eq:5.1.15}
							x^2\frac{d^2y}{dx^2}+x\frac{dy}{dx}-4y=0
							\quad y(-1)=2
							\quad \left.\frac{dy}{dx}\right|_{-1}=0
						\end {equation}
					</li>
				</ol>

				<details class="solution" id="ac9b3a00-5b58-4d10-889b-eefc35d85843">
					<summary>Solution</summary>

					<ol>
						<li>
							<div>
								First notice that <a href="#x1-24r13">(5.1.13)</a> is not in linear standard form <a href="#x1-2r1">(5.1.1)</a> but is actually in the form of <a href="#x1-22r12">(5.1.12)</a> with \( P_0(x) = x^2 \), \( P_1(x) = x \), and \( P_2(x) = -4 \), which are are all continuous everywhere \((-\infty ,\infty )\) however, we can only consider intervals where \( P_0(x) = x^2 \) has no zeros since rewriting <a href="#x1-24r13">(5.1.13)</a> in linear standard form requires dividing by \( P_0(x) \) so we obtain \( p(x) = P_1(x)/P_0(x) \) and \( q(x) = P_2(x)/P_0(x) \):
								\begin {equation*}
									\frac{d^2y}{dx^2} + \frac{1}{x}\frac{dy}{dx} - \frac{4}{x^2}y = 0
								\end {equation*}
							</div>	
							<div>	
								So now we can recognize \(p(x) = 1/x\) and since \(p(x)\) is undefined at \( x = 0 \) 
								we will only consider solutions of <a href="#x1-24r13">(5.1.13)</a> on intervals that don't contain zero \((-\infty ,0)\) and \((0,\infty )\).
								
								Then Theorem&nbsp;<a href="#x1-51">5.1.1</a> says that any IVP of <a href="#x1-24r13">(5.1.13)</a> has a unique solution on \((0,\infty )\) or on \((-\infty ,0)\) depending on the \( x \) value of the initial condition.
							</div>
						</li>	
						<li>
							<div>
								Start by checking for discontinuities in \(y_1 = x^2\) and \(y_2 = 1/x^2.\) Since \(y_1\) is a parabola it is defined everywhere with domain \((-\infty ,\infty ).\) As for \(y_2 = 1/x^2\) it is a rational function and undefined at \( x = 0 \) so its domain is \((-\infty ,0 )\) and \((0,\infty )\) 
							</div>	
							<div>
								Now calculate the second derivatives of both \(y_1\) and \(y_2\) and then substitute them into <a href="#x1-24r13">(5.1.13)</a> to verify. 
							</div>
							<div class="reading-exercise">
								Differentiate \( y_1 = x^2 \) twice using power rule
							</div> 	
							<div>
								We’ll start with \(y_1\). Differentiating:

								\begin{align*}
									y_1 = & x^2
									\Longrightarrow
									\frac{dy_1}{dx} = 2x 
									\Longrightarrow
									\frac{d^2y_1}{dx^2} = 2 \\		
								\end{align*} 
								
								Substituting into <a href="#x1-24r13">(5.1.13)</a>
								
								\begin{align*}
									x^2 \frac{d^2y}{dx^2} + x \frac{dy}{dx} - 4 y = & 0 \\
									x^2 \underset{2}{\underbrace{\frac{d^2y1}{dx^2}}} + x \underset{2x}{\underbrace{\frac{dy1}{dx}}} - 4 \underset{x^2}{\underbrace{y1}} = & 0 \\
									2x^2 + 2x^2 - 4 x^2 = & 0 \quad \checkmark
								\end{align*} 								
							</div>	
							<div class="reading-exercise">
								Differentiate \( y_2 = 1/x^2 \) twice
							</div> 	
							<div>
								Now we'll verify \(y_2\) is a solution. Differentiating:
 
								\begin{align*}
									y_2 = & \frac{1}{x^2}
									\Longrightarrow
									\frac{dy_2}{dx} = -\frac{2}{x^3}
									\Longrightarrow
									\frac{d^2y_2}{dx^2} = \frac{6}{x^4}  \\		
								\end{align*} 
								
								Substituting into <a href="#x1-24r13">(5.1.13)</a>
								 
								\begin{align*}
									x^2 \frac{d^2 y}{dx^2} + x \frac{dy}{dx} - 4y = 0 \\	
									x^2 \underset{\frac{6}{x^4}}{\underbrace{\frac{d^2 y_2}{dx^2}}} + x \underset{ -\frac{2}{x^3}}{\underbrace{\frac{dy_2}{dx}}} - 4\underset{\frac{1}{x^2}}{\underbrace{y_2}} = 0 \\	
									x^2 \cdot \frac{6}{x^4} + x \cdot  -\frac{2}{x^3} - 4\cdot \frac{1}{x^2} \overset{?}{=} & 0 \\
									\frac{6}{x^2} - \frac{2}{x^2} - \frac{4}{x^2} \overset{?}{=} & 0 \\
									\frac{6}{x^2} - \frac{6}{x^2} = & 0 \quad \checkmark
								\end{align*} 							
							</div>	
							<div>
								Therefore \( y_1 = x^2 \) is a solution of <a href="#x1-24r13">(5.1.13)</a> on \((-\infty ,\infty )\) and \( y_2 = 1/x^2 \) is a solution of <a href="#x1-24r13">(5.1.13)</a> on \((-\infty ,0 )\) and \((0,\infty )\) 
							</div>	
						
						</li>
						<li>
							<div>
								Start by checking for discontinuities. Since <a href="#x1-31r16">(5.1.16)</a> is just the sum of \( y_1 = x^2 \) and \( y_2 = 1/x^2 \) it also has a discontinuity at \( x = 0 \) so the domain is \((-\infty ,0 )\) and \((0,\infty )\) 
							</div>	
							<div>
								<a id="x1-31r16"></a>
								\begin {equation} \label {eq:5.1.16} 
									y = c_1 x^2 + \frac{c_2}{x^2}  
								\end {equation}

								Now find the second derivative of <a href="#x1-31r16">(5.1.16)</a> and substitute into <a href="#x1-24r13">(5.1.13)</a> to verify
							</div>
							<div class="reading-exercise">
								Differentiate <a href="#x1-31r16">(5.1.16)</a> twice using the power rule
							</div> 
							<div>
								<a id="x1-32r17"></a>
								\begin{align} \label {eq:5.1.17}
									\frac{dy}{dx} 
									= & \frac{d}{dx} \left( c_1 x^2 + \frac{c_2}{x^2} \right) \nonumber  \\
									= & 2 c_1 x - \frac{2c_2}{x^3}  \nonumber \\  \nonumber \\
								\frac{d^2y}{dx^2} 
									= & \frac{d}{dx} \Big( 2 c_1 x - \frac{2c_2}{x^3} \Big)	 \nonumber  \\
									= & 2 c_1 + \frac{6c_2}{x^4} 
								\end{align} 
								
								Substituting <a href="#x1-31r16">(5.1.16)</a> and <a href="#x1-32r17">(5.1.17)</a> into <a href="#x1-24r13">(5.1.13)</a>
							</div>
							<div>
								\begin{align*}
								\begin{array}{rlrrlrl}
								  \displaystyle x^2 \frac{d^2y}{dx^2}  & = & x^2 \Big( & 2c_1 & + & 6 \displaystyle\frac{c_2}{x^4} & \Big)  \\
								  \quad\quad\quad\quad \displaystyle x \frac{dy}{dx} &  = & x \Big( & 2c_1 x & - & 2 \displaystyle\frac{c_2}{x^3} & \Big)  \\
								  + \quad\quad\quad\quad \displaystyle -4y &  = & -4 \Big( & c_1 x^2 & + & \displaystyle\frac{c_2}{x^2} & \Big)  \\
								  \hline \\
								  \displaystyle x^2 \frac{d^2y}{dx^2}  & = &   & 2 c_1 x^2 & + & 6 \displaystyle\frac{c_2}{x^2} &   \\
								  \quad\quad\quad\quad \displaystyle x \frac{dy}{dx} &  = &   & 2c_1 x^2 & - & 2 \displaystyle\frac{c_2}{x^2} &   \\
								  + \quad\quad\quad\quad \displaystyle -4y &  = &   & -4 c_1 x^2 & - & 4 \displaystyle\frac{c_2}{x^2} &   \\
								  \hline \\		
								  x^2 \displaystyle \frac{d^2y}{dx^2}
								  	+ x \frac{dy}{dx}
									- 4y 			
										&  = &   & \overset{2+2-4}{\overbrace{0}} c_1 x^2 & + & \overset{6-2-4}{\overbrace{0}} \displaystyle\frac{c_2}{x^2} &   \\ \\	
									x^2 \displaystyle \frac{d^2y}{dx^2}
										+ x \frac{dy}{dx}
									  	- 4y 			
										  &  \overset{\checkmark}{=} & 0  &   \\					  
								\end{array}
							  \end{align*}      
							</div>	
							<div>
								Therefore \( \displaystyle y = c_1 x^2 + \frac{c_2}{x^2} \) is a solution of <a href="#x1-24r13">(5.1.13)</a> on \((-\infty ,0 )\) and \((0,\infty )\)  								
							</div>	
							<div>
								<figure>
									<div
									>
										<iframe src="https://www.desmos.com/calculator/5qna6hhxnm?embed" width="400" height="250" frameborder="0"></iframe>
									</div>
									<figcaption>
										<span class="id">Figure&nbsp;5.1.1(a)</span>
										<span class="content">
											Graph of \( \displaystyle y = c_1 x^2 + \frac{c_2}{x^2} \) solution of <a href="#x1-24r13">(5.1.13)</a> on \((-\infty ,0 )\) and \((0,\infty )\) with varying constant values 
										</span>
									</figcaption>
								</figure>								
							</div>	
						</li>
						<li>
							<div>
								In Part 3 we already verified \( \displaystyle y = c_1 x^2 + \frac{c_2}{x^2} \) is a solution of <a href="#x1-24r13">(5.1.13)</a> and we computed its derivative so we have:

								\begin{align*}
									y = & c_1 x^2 + \frac{c_2}{x^2} \\ \nonumber \\
									\frac{dy}{dx} = & 2 c_1 x - \frac{2c_2}{x^3} \nonumber \\
								\end{align*} 
							</div>
							<div>
								To solve the IVP we simply apply the initial conditions 
								\( y(1)=2 \) and \( \displaystyle \left.\frac{dy}{dx}\right|_{1}=0 \)
							</div>	
							<div>
								\begin{align*}
									y(x) = & c_1 x^2 + \frac{c_2}{x^2} \\ 
									y(1) = & c_1 \cdot 1^2 + \frac{c_2}{1^2} \\  
									2    = & c_1           + c_2 \\  \\ 
		
									\frac{dy}{dx} = & 2 c_1 x - \frac{2c_2}{x^3}   \\
									\left.\frac{dy}{dx}\right|_{1} = & 2 c_1 \cdot 1 - \frac{2c_2}{1^3}   \\
									0							   = & 2 c_1 	     - 2c_2   \\
									0							   = &   c_1 	     - c_2   \\
								\end{align*} 								
							</div>	
							<div>
								And we obtain a system of equations which we can solve by adding them together to find \(c_1\) then substituting that back to find \(c_2\)
								\begin{align*}
								\left\{
									\begin{array}{r l}
										c_1 + c_2 = & 2 \\
										c_1 - c_2 = & 0 \\
									\end{array}
								\right.	
								=
								\left\{
									\begin{array}{r l}
										c_1 + c_2 = & 2 \\
										2c_1 = & 2 \\
									\end{array}
								\right.	
								=
								\left\{
									\begin{array}{r l}
										1 + c_2 = & 2 \\
										c_1 = & 1 \\
									\end{array}
								\right.		
								=
								\left\{
									\begin{array}{r l}
										c_2 = & 1 \\
										c_1 = & 1 \\
									\end{array}
								\right.									 		
							\end{align*}								
							</div>	

							<div>
								Substituting \(c_1 = 1\) and \(c_2 = 1 \) into <a href="#x1-31r16">(5.1.16)</a> we obtain our unique solution to the IVP <a href="#x1-28r14">(5.1.14)</a> on IOV \((0,\infty )\) since our initial value of \(x\) is \(1\)
								\[
									y=  x^2 + \frac{1}{x^2}
								\]
							</div>	
							<div class="reading-exercise">
								Verify \( y=  x^2 + \frac{1}{x^2} \) is a solution to <a href="#x1-24r13">(5.1.13)</a> 
							</div> 
							<div>
								<figure>
									<div
									>
										<iframe src="https://www.desmos.com/calculator/rgbyh3sm0j?embed" width="400" height="250" frameborder="0"></iframe>
									</div>
									<figcaption>
										<span class="id">Figure&nbsp;5.1.1(b)</span>
										<span class="content">
											Unique solution
											\( \displaystyle y = x^2 + \frac{1}{x^2} \)
											of <a href="#x1-28r14">(5.1.14)</a> on \((0,\infty )\) 
										</span>
									</figcaption>
								</figure>								
							</div>	
						

						</li>
						<li>
							<div>
								Again in Part 3 we already verified \( \displaystyle y = c_1 x^2 + \frac{c_2}{x^2} \) is a solution of <a href="#x1-24r13">(5.1.13)</a> and we computed its derivative so we have:

								\begin{align*}
									y = & c_1 x^2 + \frac{c_2}{x^2} \\ \nonumber \\
									\frac{dy}{dx} = & 2 c_1 x - \frac{2c_2}{x^3} \nonumber \\
								\end{align*} 
							</div>
							<div>
								To solve the IVP we simply apply the initial conditions 
								\( y(-1)=2 \) and \( \displaystyle \left.\frac{dy}{dx}\right|_{-1}=0 \)
							</div>	
							<div>
								\begin{align*}
									y(x) = & c_1 x^2 + \frac{c_2}{x^2} \\ 
									y(-1) = & c_1 \cdot (-1)^2 + \frac{c_2}{(-1)^2} \\  
									2    = & c_1           + c_2 \\  \\ 
		
									\frac{dy}{dx} = & 2 c_1 x - \frac{2c_2}{x^3}   \\
									\left.\frac{dy}{dx}\right|_{-1} = & 2 c_1 \cdot (-1) - \frac{2c_2}{(-1)^3}   \\
									0							   = & - 2 c_1 	     + 2c_2   \\
									0							   = &  - c_1 	     + c_2   \\
									0							   = &  c_1 	     - c_2   \\
								\end{align*} 								
							</div>	
							<div>
								And we obtain a system of equations which is the same system we got in Part 4 and we already solved
								\begin{align*}
								\left\{
									\begin{array}{r l}
										c_1 + c_2 = & 2 \\
										c_1 - c_2 = & 0 \\
									\end{array}
								\right.		
								\quad
								=
								\left\{
									\begin{array}{r l}
										c_1 = & 1 \\
										c_2 = & 1 \\
									\end{array}
								\right.									 		
							\end{align*}								
							</div>	

							<div>
								Again we obtain our unique solution to the IVP <a href="#x1-28r14">(5.1.14)</a> but this time on IOV \( (-\infty ,0 )\) since our initial value of \(x\) is \(-1\)

								\[
									y=  x^2 + \frac{1}{x^2}
								\]
							</div>	
							<div>
								<figure>
									<div
									>
										<iframe src="https://www.desmos.com/calculator/nrjm8eb71x?embed" width="400" height="250" frameborder="0"></iframe>
									</div>
									<figcaption>
										<span class="id">Figure&nbsp;5.1.1(c)</span>
										<span class="content">
											Unique solution
											\( \displaystyle y = x^2 + \frac{1}{x^2} \)
											of <a href="#x1-30r15">(5.1.15)</a> on \( (-\infty ,0 )\) 
										</span>

									</figcaption>
								</figure>								
							</div>								
						</li>						
					</ol>

				</details>
			</div>

			<br>

			<!--Blurb after Example 3-->
			<div class="indent" id="467c5185-9cf7-47c1-9bbe-0f1bce3be4a1">
				Although the <em>formulas</em>
				for the solutions of <a href="#x1-28r14">(5.1.14)</a> and <a href="#x1-30r15">(5.1.15)</a> are both \(y=x^2+1/x^2\), we should not conclude that these two IVPs have the same solution. Remember that a solution of an IVP is defined on an interval that contains the initial point; therefore, the solution of <a href="#x1-28r14">(5.1.14)</a> is \(y=x^2+1/x^2\) on the interval \((0,\infty )\), which contains the initial point \(x_0=1\), while the solution of <a href="#x1-30r15">(5.1.15)</a> is \(y=x^2+1/x^2\) on the interval \((-\infty ,0)\), which contains the initial point \(x_0=-1\)
			</div>
			<div>
				<figure>
					<div
					>
						<iframe src="https://www.desmos.com/calculator/zmnwbbikes?embed" width="400" height="250" frameborder="0"></iframe>
					</div>
					<figcaption>
						<span class="id">Figure&nbsp;5.1.1(d)</span>
						<span class="content">
							Unique solutions
							of <a href="#x1-28r14">(5.1.14)</a> on \((0,\infty )\) 
							and
							of <a href="#x1-30r15">(5.1.15)</a> on \( (-\infty ,0 )\) 

						</span>
					</figcaption>
				</figure>								
			</div>				

		</details>		

		<!--Linear Independence-->
		<details id="935e85a1-5159-413b-a779-b952361b8b4c">
			<summary>Linear Independence</summary>

			<div id="dcf91a05-365b-4abb-8756-c0df04e1205f">
				If \(y_1\) and \(y_2\) are defined on an interval \((a,b)\) and \(c_1\)
				and \(c_2\) are constants, then \[ y=c_1y_1+c_2y_2 \] is a
				<span class="vocab">linear combination</span>
				of \(y_1\) and \(y_2\). For example, \(y=2\cos x+7 \sin x\) is a linear
				combination of \(y_1= \cos x\) and \(y_2=\sin x\), with \(c_1=2\) and
				\(c_2=7\).
			</div>
			<div class="indent" id="96180d4c-a867-498f-89dc-41a8784432c0">
				The next theorem states a fact that we’ve already verified in
				Examples&nbsp;<a href="#x1-61">5.1.1</a>,
				<a href="#x1-142">5.1.2</a>, and
				<a href="#x1-233">5.1.3</a>.
			</div>

			<div class="theorem" id="c5e939e5-3da8-4282-941e-dfa60fa89893">
				<div>
					<span class="head">Theorem&nbsp;5.1.2</span>
					<a id="x1-332"></a>
					If \(y_1\) and \(y_2\) are solutions of the homogeneous equation on \((a,b)\)
					
					<a id="x1-34r18"></a>
					\begin {equation} \label {eq:5.1.18} 
						\frac{d^2y}{dx^2}+p(x)\frac{dy}{dx}+q(x)y=0 
					\end {equation}
					
					Then any linear combination of \(y_1\) and \(y_2\) is also a solution of <a href="#x1-34r18">(5.1.18)</a> on \((a,b)\)

					<a id="x1-35r19"></a>
					\begin {equation} \label {eq:5.1.19} 
						y=c_1y_1+c_2y_2 
					\end {equation}
					
				</div>
			</div>

			<br>

			<div class="reading-exercise" id="c5e939e5-3da8-4282-941e-dfa60fa89893reaw">
				Differentiate \( y=c_1y_1+c_2y_2 \) twice to find \(\frac{dy}{dx}\) and \(\frac{d^2y}{dx^2}\) 
			</div> 

			<div class="reading-exercise" id="c5e939e5-3da8-4282-941e-dfa60fa89893feqw">
				Substitute \(y_1\) and \(y_2\) into <a href="#x1-34r18">(5.1.18)</a>
			</div> 

			<div class="reading-exercise" id="c5e939e5-3da8-4282-941e-dfa60fa898fdsaf93">
				Verify \( y=c_1y_1+c_2y_2 \) is a solution of <a href="#x1-34r18">(5.1.18)</a>
			</div> 

			<br>
			<br>

			<div class="indent" id="cd388051-210f-4db7-9cdd-f4d32479734e">
				We say that \(\{y_1,y_2\}\) is a <span class="vocab">fundamental set</span> of solutions of <a href="#x1-34r18">(5.1.18)</a> on \((a,b)\) if every solution of <a href="#x1-34r18">(5.1.18)</a> on \((a,b)\) can be written as a linear combination of \(y_1\) and \(y_2\) as in <a href="#x1-35r19">(5.1.19)</a>. In this case we say that <a href="#x1-35r19">(5.1.19)</a> is <span class="vocab">general solution</span> of <a href="#x1-34r18">(5.1.18)</a> on \((a,b)\)
			</div>
			<div id="0e43a534-0f0b-40d8-93f0-3c8c894215a3">
				We need a way to determine whether a given set \(\{y_1,y_2\}\) of solutions of <a href="#x1-34r18">(5.1.18)</a> is a <span class="vocab">fundamental set</span>. The next definition will enable us to state necessary and sufficient conditions for this.
			</div>
			<div class="indent" id="8886d612-039f-40e8-908a-2153ca66e3a8">
				We say that two functions \(y_1\) and \(y_2\) defined on an interval \((a,b)\) are <span class="vocab">linearly independent</span> on \((a,b)\) if neither is a constant multiple of the other on \((a,b)\). In particular, this means that neither can be the trivial solution \( y \equiv 0 .\) When \(y_1\) and \(y_2\) are <span class="vocab">linearly independent</span> on \((a,b),\) we also say that the set \(\{y_1,y_2\}\) is <span class="vocab">linearly independent</span> on \((a,b)\).
			</div>
			<div class="theorem" id="a5aa20a7-255d-4d9e-978c-b2d9d72509d2">
				<div>
					<span class="head">Theorem&nbsp;5.1.3</span>
					<a id="x1-363"></a>
					Suppose \(p(x)\) and \(q(x)\) are continuous on \((a,b).\) Then a set
					\(\{y_1,y_2\}\) of solutions of \begin {equation} \label {eq:5.1.20}
					\frac{d^2y}{dx^2}+p(x)\frac{dy}{dx}+q(x)y=0 \end {equation}
					<a id="x1-37r20"></a>
					on \((a,b)\) is a <span class="vocab">fundamental set</span> if and only if \(\{y_1,y_2\}\) is
					<span class="vocab">linearly independent</span> on \((a,b)\)
				</div>
			</div>

			<div class="indent" id="40fabbc2-5ca7-4e81-a280-3706257d9809">
				New let’s interpret Theorem&nbsp;<a href="#x1-363">5.1.3</a>
				in terms of Examples&nbsp;<a href="#x1-61">5.1.1</a>,
				<a href="#x1-142">5.1.2</a>, and
				<a href="#x1-233">5.1.3</a>.
			</div>

			<div class="example" id="x1-384" style="--ex-number: 4">
				<ol>
					<li>	
						Find the general solution of <a href="#x1-7r4">(5.1.4)</a>
						from Example <a href="#x1-61">5.1.1</a> 
					</li>
					<li>
						Find the general solution of <a href="#x1-15r8">(5.1.8)</a>
						from Example <a href="#x1-142">5.1.2</a> 
					</li>
					<li>
						Find the general solution of <a href="#x1-24r13">(5.1.13)</a>
						from Example <a href="#x1-233">5.1.3</a> 
					</li>
				</ol>

				<details class="solution">
					<summary>Solution</summary>
					<ol>
						<li>
							<div>
								In Example <a href="#x1-61">5.1.1</a> we already established that \(y_1 = e^x\) and \(y_2 = e^{-x}\) are continuous on \((-\infty ,\infty ).\) Now we would like to know if \(y_1 = e^x\) and \(y_2 = e^{-x}\) are <span class="vocab">linearly independent</span> 

								\[
									\frac{y_1}{y_2} = \frac{e^x}{e^{-x}}=e^{2x}
								\]
								
								Since \( y_1/y_2 = e^{2x} \) is not constant, then neither function is a constant multiple of the other meaning \(y_1 = e^x\) and \(y_2 = e^{-x}\) are <span class="vocab-repeat">linearly independent</span> and therefore from Theorem Theorem&nbsp;<a href="#x1-363">5.1.3</a> \(\{y_1,y_2\} = \{e^x,e^{-x}\}  \) represents a <span class="vocab">fundamental set</span> of solutions of <a href="#x1-7r4">(5.1.4)</a> and that  

								\[
									y = c_1 y_1 + c_2 y_2
									= c_1 e^x + c_2 e^{-x}
								\]

								is the <span class="vocab">general solution</span> of <a href="#x1-7r4">(5.1.4)</a> on \((-\infty ,\infty )\)
							</div>
							<div>
							</div>
						</li>
						<li>
							
							<div>
								In Example <a href="#x1-142">5.1.2</a> we already established that \(y_1 = \cos \omega x\) and \(y_2 = \sin \omega x\) are continuous on \((-\infty ,\infty ).\) Now we would like to know if \(y_1 =  \cos \omega x \) and \(y_2 = \sin \omega x \) are <span class="vocab">linearly independent</span> 

								\[
									\frac{y_1}{y_2} = \frac{\cos \omega x}{\sin \omega x}=\tan \omega x
								\]
								
								Since \( y_1/y_2 = \tan \omega x\) is not constant, then neither function is a constant multiple of the other meaning \(y_1 = \cos \omega x\) and \(y_2 = \sin \omega x\) are <span class="vocab-repeat">linearly independent</span> and therefore from Theorem Theorem&nbsp;<a href="#x1-363">5.1.3</a> \(\{y_1,y_2\} = \{\cos \omega x,\sin \omega x\}  \) represents a <span class="vocab">fundamental set</span> of solutions of <a href="#x1-15r8">(5.1.8)</a> and that  

								\[
									y = c_1 y_1 + c_2 y_2
									= c_1 \cos \omega x + c_2 \sin \omega x
								\]
								is the <span class="vocab">general solution</span> of <a href="#x1-15r8">(5.1.8)</a> on \((-\infty ,\infty )\)
							</div>
							<div>
							</div>							
						</li>
						<li>
							<div>
								In Example <a href="#x1-233">5.1.3</a> we already established that \(y_1 = x^2\) is continuous on \((-\infty ,\infty )\) and \(y_2 = 1/x^2\) is continuous on \((-\infty ,0 )\) and \((0 ,\infty ).\) Now we would like to know if \(y_1 = x^2\) and \(y_2 = 1/x^2\) are <span class="vocab">linearly independent</span> 

								\[
									\frac{y_1}{y_2} = \frac{x^2}{1/x^2}=x^4
								\]
								
								Since \( y_1/y_2 = x^4 \) is not constant, then neither function is a constant multiple of the other meaning \(y_1 = x^2\) and \(y_2 = 1/x^2\) are <span class="vocab-repeat">linearly independent</span> and therefore from Theorem Theorem&nbsp;<a href="#x1-363">5.1.3</a> \(\{y_1,y_2\} = \{x^2,1/x^2\}  \) represents a <span class="vocab">fundamental set</span> of solutions of <a href="#x1-15r8">(5.1.8)</a> and that  

								\[
									y = c_1 y_1 + c_2 y_2
									= c_1 x^2 + \frac{c_2}{x^2}
								\]
								is the <span class="vocab">general solution</span> of <a href="#x1-24r13">(5.1.13)</a> on  \((-\infty ,0 )\) and \((0 ,\infty )\)
							</div>							
						</li>
					</ol>
				</details>

			</div>			
		</details>
		
		<!--Wronskian-->
		<details id="a726b580-934a-4a2f-9371-d9cc658d9037">
			<summary>Wronskian</summary>
		
			<div id="b0ad295a-f305-47cc-ab47-ac09b8cee963">
				To motivate a result that we need in order to prove Theorem&nbsp;<a href="#x1-363">5.1.3</a>, let’s find out what is required to prove that \(\{y_1,y_2\}\) is a fundamental set of solutions of <a href="#x1-37r20">(5.1.20)</a> on \((a,b)\). Let \(x_0\) be an arbitrary point in \((a,b)\) and let \(y\) be an arbitrary solution of <a href="#x1-37r20">(5.1.20)</a> then \(y\) is the unique solution to the IVP on \((a,b)\)
				
				<a id="x1-42r21"></a>
				\begin {equation} \label {eq:5.1.21} 
					\frac{d^2y}{dx^2}+p(x)\frac{dy}{dx}+q(x)y=0
					\quad y(x_0)=k_0
					\quad \left.\frac{dy}{dx}\right|_{x_0}=k_1 
				\end {equation}
				
				Since \(y\) is the unique solution to the IVP this means that evaluating \(y\) at the initial \(x\) value \(x_0\) results in the number \(k_0\) and evaluating \(dy/dx\) at the initial \(x\) value \(x_0\) results in  the number \(k_1.\)
			</div>
			<div>	
				Theorem&nbsp;<a href="#x1-51">5.1.1</a> guarantees the existence of a solution to all second order linear differential equations so we know that <a href="#x1-42r21">(5.1.21)</a> has a solution for any values of \(k_0\) and \(k_1.\) 
			
				Theorem&nbsp;<a href="#x1-332">5.1.2</a> tells us that if \(y_1\) and \(y_2\) are solutions of <a href="#x1-37r20">(5.1.20)</a> then any linear combination of them is also a solution
				
				\[y = c_1 y_1 + c_2 y_2\] 
				
				which is the general solution of <a href="#x1-37r20">(5.1.20)</a> if and only if \(\{y_1,y_2\}\) is a fundamental set of solutions of <a href="#x1-37r20">(5.1.20)</a>.
				
			</div>
			<div>
				So we will try to find a way to express the solution to the IVP <a href="#x1-42r21">(5.1.21)</a> in terms of \(y = c_1 y_1 + c_2 y_2\) which will show it is the general solution of <a href="#x1-37r20">(5.1.20)</a> and that therefore \(\{y_1,y_2\}\) is a fundamental set. Differentiating \(y = c_1 y_1 + c_2 y_2\)  we obtain

				\[\frac{dy}{dx}=c_1\frac{dy_1}{dx}+c_2\frac{dy_2}{dx}\] 
			</div>
			<div>	

				And evaluating at our initial conditions \( y(x_0)=k_0 \) and \( \left.\frac{dy}{dx}\right|_{x_0}=k_1 \) we obtain the following system of equations:
				
				<a id="x1-43r22"></a>
				\begin {equation} \label {eq:5.1.22} 
					\left\{
					\begin {array}{rcl}
						c_1 y_1(x_0) + c_2 y_2(x_0) & = & k_0\\ \\
						c_1 \displaystyle \left.\displaystyle\frac{dy_1}{dx}\right|_{x_0} + c_2 \left.\displaystyle\frac{dy_2}{dx}\right|_{x_0} & = & k_1 
					\end {array} 
					\right.
				\end {equation}

				Which we will solve for the constants \(c_1\) and \(c_2\) to obtain the particular solution of the IVP <a href="#x1-42r21">(5.1.21)</a>. So if we can solve this system of equations <a href="#x1-43r22">(5.1.22)</a> then that means we are able to express the solution to the IVP  <a href="#x1-42r21">(5.1.21)</a> as a linear combination of \( y_1 \) and \(y_2\) meaning that \(\{y_1,y_2\}\) is a fundamental set of solutions for <a href="#x1-37r20">(5.1.20)</a>.
			</div>	 

			<div>
				
				To simplify our computations in solving for \(c_1\) and \(c_2\) we will suppress writing the \( x_0 \) evaluation of the solutions, so we will write \( y_1 \) to mean \( y_1(x_0) \) and \( y_2 \) to mean \( y_2(x_0).\) And similarly for the derivatives, we will write \( {dy_1}/{dx} \) to mean \( {dy_1}/{dx} \) evaluated at \( x_0 \) and \( {dy_2}/{dx} \) to mean \( {dy_2}/{dx}\) evaluated at \( x_0 .\) It is important to remember that the \(x_0\) evaluations are still there but we just won't be writing them so that our computations are easier to read so we will write our system <a href="#x1-43r22">(5.1.22)</a> as:

				\begin {equation*} 
					\left\{
					\begin {array}{rcrcrr}
						c_1 y_1 &+& c_2 y_2 & = & k_0 & \quad\quad\quad (1) \\ \\
						c_1 \displaystyle\frac{dy_1}{dx} &+& c_2 \displaystyle\frac{dy_2}{dx} & = & k_1 & \quad\quad\quad (2) \\
					\end {array} 
					\right.
				\end {equation*}
			</div>

			<br>

			<div class="indent" id="2286a18e-657e-4f73-8364-8c37502e96b5">
				As our goal is to solve the system for the constants \(c_1\) and \(c_2,\) we will start by solving for \(c_1\) by eliminating the \(c_2\) term. To do this, we multiply the first equation (1) by \( \displaystyle\frac{dy_2}{dx} \) and the second (2) by \( y_2 \)

				\begin {equation*} 
					\left\{
					\begin {array}{rrrclr}
						c_1 y_1 \displaystyle\frac{dy_2}{dx} &+& c_2 y_2 \displaystyle\frac{dy_2}{dx} & = & k_0 \displaystyle\frac{dy_2}{dx} & \quad\quad\quad (3) \\ \\
						c_1 y_2 \displaystyle\frac{dy_1}{dx}  &+& c_2 y_2 \displaystyle\frac{dy_2}{dx}  & = & k_1 y_2 & \quad\quad\quad (4) \\
					\end {array} 
					\right.
				\end {equation*}
				
				Now subtracting (4) from (3) yields:

				\begin {equation*} 
					\begin {array}{rclclr}
						c_1 y_1 \displaystyle\frac{dy_2}{dx} & 
							+ &
							c_2 y_2 \displaystyle\frac{dy_2}{dx} &
							 = &
							k_0 \displaystyle\frac{dy_2}{dx} &
							\quad \\
						- \quad \quad \quad \Bigg( \quad c_1 y_2 \displaystyle\frac{dy_1}{dx} &
							+ &
							c_2 y_2 \displaystyle\frac{dy_2}{dx} &
							= &
							k_1 y_2 \quad \Bigg) & 
							\quad \\
						\hline \\
						\Big( c_1 y_1 \displaystyle\frac{dy_2}{dx} - c_1 y_2 \displaystyle\frac{dy_1}{dx} \Big) & 
							+ &
							\quad \quad 0 &
							 = &
							k_0 \displaystyle\frac{dy_2}{dx} - k_1 y_2 &
							\quad \\
						\end {array} 
				\end {equation*}

			</div>

			<br>

			<div>
				Factoring out the \(c_1\) we have

				<a id="x1-44r23"></a>
				\begin {equation} 
					c_1 
					\left(  y_1 \displaystyle\frac{dy_2}{dx} - \displaystyle\frac{dy_1}{dx} y_2\right) 
							 = k_0 \displaystyle\frac{dy_2}{dx} - k_1 y_2 \quad\quad\quad 
				\end {equation}
			</div>

			<br>

			<div>
				Which we can solve for \( c_1 \) by dividing by the quantity \( \left(  y_1 \frac{dy_2}{dx} - \frac{dy_1}{dx} y_2\right) \) assuming it is nonzero, therefore we have 

				\begin {equation*} 
					c_1 =
						\left.\\[30pt]
						\frac{\left( k_0 \displaystyle\frac{dy_2}{dx} - k_1 y_2 \right)}{\left( y_1 \displaystyle\frac{dy_2}{dx} - \displaystyle\frac{dy_1}{dx} y_2\right) } 
						\,
						\right|_{x_0}
				\end {equation*}

				While we have found \(c_1\) it is worth noting that these expressions are fairly complicated to read and write and one common practice is to instead write them in terms of determinants which makes it more readable. As a reminder the definition of a determinant is:

				\begin {equation*} 
					\det
					\left(
						\begin{array}{cc}
							A & B \\
							C & D
						\end{array}
					\right)
					= AD - CB
				\end {equation*}

				Rewriting the numerator and denominator of \(c_1\) as determinants we have

				\begin {equation*} 
					c_1 =
						\left. \\[40pt]
						\frac{\,
							\det
							\left(
								\begin{array}{cc}
									\, k_0 & y_2   \\
									\, k_1 & \displaystyle \frac{dy_2}{dx}  
								\end{array}
							\right)
							\,}{\,
							\det	
							\left(
								\begin{array}{cc}
									y_1   & y_2   \\
									\displaystyle \frac{dy_1}{dx}   & \displaystyle \frac{dy_2}{dx}  
								\end{array}
							\right)
							\,} 
							\,
							\right|_{x_0}
				\end {equation*}

				
			</div>
			<div>	
				One more point to make before moving on is that we refer to the expression in the denominator as the
				<span class="vocab">Wronskian determinant</span> or just the <span class="vocab">Wronskian</span> of \( \{y_1, y_2\} \) denoted \( W[y_1, y_2](x) \) which is often simplified to \( W(x) \) when it is well-understood what \( y_1 \) and \( y_2 \) are, so the <span class="vocab-repeat">Wronskian</span> as a function of \(x\) is given by

				\begin {equation*} 
					W(x) = 
					\det
					\left(
						\begin{array}{cc}
							y_1   & y_2   \\
							\displaystyle \frac{dy_1}{dx}   & \displaystyle \frac{dy_2}{dx}
						\end{array}
					\right)
					=  y_1 \displaystyle\frac{dy_2}{dx} - \displaystyle\frac{dy_1}{dx} y_2
				\end {equation*}

				And \( W(x_0) \) will indicate the <span class="vocab-repeat">Wronskian</span> is evaluated at the \(x\) value of \(x_0,\) as is the case for our expression for \( c_1 \)

				\begin {equation*} 
					W(x_0) = 
					\left. \\[20pt]
					\det
					\left(
						\begin{array}{cc}
							y_1   & y_2   \\
							\displaystyle \frac{dy_1}{dx}   & \displaystyle \frac{dy_2}{dx}
						\end{array}
					\right)
					\,\,
					\right|_{x_0}
					=  \left. \\[15pt] \left[ 
							y_1 \displaystyle\frac{dy_2}{dx} - \displaystyle\frac{dy_1}{dx} y_2
						\right] \, \right|_{x_0}
				\end {equation*}

			</div>
			<div>
				Knowing what the <span class="vocab-repeat">Wronskian</span> is helps us write the expression for \(c_1\) even simpler by expressing the denominator as the <span class="vocab">Wronskian</span> and the numerator as a determinant, often written as:

				\begin {equation*} 
					c_1 =
						\frac{1}{W(x_0)}
						\cdot
							\left. \\[20pt]
							\det
							\left(
								\begin{array}{cc}
									\, k_0 & y_2 \\
									\, k_1 & \displaystyle \frac{dy_2}{dx}
								\end{array}
							\right)
							\,\,
							\right|_{x_0}
				\end {equation*}

			</div>	


			<br>
			<br>
			<br>

			<div class="indent" id="2286a18e-657e-4f73-8364-8c37502e96b5fdsadfs">
				Now that was a lot of new notation! And we have a nice expression for \(c_1.\) But we were trying to solve the system <a href="#x1-43r22">(5.1.22)</a>, and so we still need to find an expression for \(c_2.\) Let's go back to our original equations (1) and (2) and try to solve for \(c_2\) this time by eliminating the \(c_1\) term. 
				
				
				\begin {equation*} 
					\left\{
					\begin {array}{rcrcrr}
						c_1 y_1 &+& c_2 y_2 & = & k_0 & \quad\quad\quad (1) \\ \\
						c_1 \displaystyle\frac{dy_1}{dx} &+& c_2 \displaystyle\frac{dy_2}{dx} & = & k_1 & \quad\quad\quad (2) \\
					\end {array} 
					\right.
				\end {equation*}
			</div>	

			<br>
				
			<div id="491f663f-16b0-4c97-9bb9-a330a67e9990">
				Last time we multiplied by \( {dy_2}/{dx} \) and \( y_2 \) but to eliminate \(c_1\) we will multiply the first equation (1) in by \( \displaystyle\frac{dy_1}{dx} \) and the second (2) by \( y_1 \)

				\begin {equation*} 
					\left\{
					\begin {array}{rcrrlr}
						c_1 y_1 \displaystyle\frac{dy_1}{dx} &+&  c_2 y_2 \displaystyle\frac{dy_1}{dx}   & = & k_0 \displaystyle\frac{dy_1}{dx} & \quad\quad\quad (5) \\ \\
						c_1 y_1 \displaystyle\frac{dy_1}{dx}  &+& c_2 y_1  \displaystyle\frac{dy_2}{dx}  & = & k_1 y_1 & \quad\quad\quad (6) \\
					\end {array} 
					\right.
				\end {equation*}
				
				Now subtracting (5) from (6) yields:

				\begin {equation*} 
					\begin {array}{rcccl}
						c_1 y_1 \displaystyle\frac{dy_1}{dx} & + & c_2 y_1 \displaystyle\frac{dy_2}{dx} & = & k_1 y_1  & \quad \\
						- \quad \quad \quad \Bigg( \quad c_1 y_1 \displaystyle\frac{dy_1}{dx} & + & c_2 y_2 \displaystyle\frac{dy_1}{dx} & = & k_0 \displaystyle\frac{dy_1}{dx}  \quad \Bigg) & \quad \\
						\hline \\
						0 & + & \Big( c_2 y_1 \displaystyle\frac{dy_2}{dx} - c_2 y_2 \displaystyle\frac{dy_1}{dx} \Big) & = & k_1 y_1 - k_0 \displaystyle\frac{dy_1}{dx}  & \quad \\
						\end {array} 
				\end {equation*}
			</div>
			
			<div>	
				Factoring out the \(c_2\) we have
				
				<a id="x1-45r24"></a>
				\begin {equation} 
					c_2 
					\left(  y_1 \displaystyle\frac{dy_2}{dx} - \displaystyle\frac{dy_1}{dx} y_2 \right) 
							 = k_1 y_1 - k_0 \displaystyle\frac{dy_1}{dx} \quad\quad\quad 
				\end {equation}
			</div>	

			<br>
			<div>
				Which again we can solve for \( c_2 \) by dividing by the quantity same \( \left(  y_1 \frac{dy_2}{dx} - \frac{dy_1}{dx} y_2\right) \) again, so assuming it is nonzero we get

				\begin {equation*} 
					c_2 =
						\left.\\[30pt]
						\frac{\left( k_1 y_1 - k_0 \displaystyle\frac{dy_1}{dx} \right)}{\left( y_1 \displaystyle\frac{dy_2}{dx} - \displaystyle\frac{dy_1}{dx} y_2\right) } 
						\,
						\right|_{x_0}
				\end {equation*}

				And we already know we can rewrite these expressions in terms of determinants:

				\begin {equation*} 
					c_2 =
						\left. \\[40pt]
						\frac{\,
							\det
							\left(
								\begin{array}{cc}
									\, y_1 & k_0 \,    \\
									\, \displaystyle \frac{dy_1}{dx} & k_1 \,  
								\end{array}
							\right)
							\,}{\,
								\det
							\left(
								\begin{array}{cc}
									y_1   & y_2   \\
									\displaystyle \frac{dy_1}{dx}   & \displaystyle \frac{dy_2}{dx}  
								\end{array}
							\right)
							\,} 
							\,
							\right|_{x_0}
				\end {equation*}

				
			</div>
			<div>	
				And again we can recognize the denominator as the <span class="vocab">Wronskian</span> of \( \{y_1, y_2\} \) which we will write as \( W(x_0) \) indicating it is evaluated at the \(x\) value \(x_0\) so we can simplify the expression for \(c_2\) as

				\begin {equation*} 
					c_2 =
						\frac{1}{W(x_0)} \cdot
							\left. \\[20pt]
							\det
							\left(
								\begin{array}{cc}
									\, y_1 & k_0 \,    \\
									\, \displaystyle \frac{dy_1}{dx} & k_1 \,  
								\end{array}
							\right)
							\,\,
							\right|_{x_0}
				\end {equation*}

			</div>	





			<br>
			<br>

			<div class="indent" id="8ee3123e-3632-49c5-a613-55e5485a51f9">
			
		</div>
		<br>
		<div>		

			</div>
			<div class="stepline"> 
				By solving the system <a href="#x1-43r22">(5.1.22)</a> we were able to find expressions for both constants \(c_1\) and \(c_2\) in terms any values of \(k_0\) and \(k_1,\) as long as the <span class="vocab">Wronskian</span> is nonzero <a href="#x1-46r25">(5.1.25)</a>

				<a id="x1-46r25"></a>
				\begin{equation} \label{5.1.25}
					W(x) = 
						\det
						\left(
							\begin{array}{cc}
								y_1   & y_2   \\
								\displaystyle \frac{dy_1}{dx}   & \displaystyle \frac{dy_2}{dx}  
							\end{array}
						\right)
					=y_1 \displaystyle\frac{dy_2}{dx} - \displaystyle\frac{dy_1}{dx} y_2 \ne 0
				\end{equation}

				This means that whenever the <span class="vocab-repeat">Wronskian</span> is nonzero then the IVP  <a href="#x1-42r21">(5.1.21)</a>	
				
				\begin{align} 
					\frac{d^2y}{dx^2}+p(x)\frac{dy}{dx}+q(x)y= & 0
						\quad y(x_0)=k_0
						\quad \left.\frac{dy}{dx}\right|_{x_0}=k_1 \nonumber \\
				\end{align} 

				Always has the unique solution 

				\begin{align} 
					y = & c_1 y_1 + c_2 y_2 \nonumber \\
				\end{align} 

				Where \( \{y_1,y_2\} \) represents a fundamental set of solutions and the constants are defined as

				\begin{align} 
					c_1 = & 
						\frac{1}{W(x_0)}
								\cdot
								\left. \\[20pt]
								\det
								\left(
									\begin{array}{cc}
										\, k_0 & y_2 \\
										\, k_1 & \displaystyle \frac{dy_2}{dx}
									\end{array}
								\right)
								\,\,
								\right|_{x_0}
					\quad\quad\quad			
					c_2 = 
						\frac{1}{W(x_0)}
							\cdot
							\left. \\[20pt]
							\left(
								\begin{array}{cc}
									\, y_1 & k_0 \,    \\
									\, \displaystyle \frac{dy_1}{dx} & k_1 \,  
								\end{array}
							\right)
							\,\,
							\right|_{x_0} \\ \nonumber \\
				\end{align} 


				<div class="footnote">
					*You may recognize <a href="#x1-47r26">(5.1.26)</a> as <span class="vocab">Cramer’s rule</span> for the system <a href="#x1-43r22">(5.1.22)</a> 
				</div>	
			</div>
			<br>
			<br>

			<div class="indent" id="1b60beb3-675b-4f69-9c24-ff298a19080e">
				Our solution so far was dependent on the <span class="vocab-repeat">Wronskian</span> being nonzero so we have left to consider the situation when the <span class="vocab-repeat">Wronskian</span> is equal to zero. 

				When \( W[y_1,y_2](x) =  0 \) then <a href="#x1-44r23">(5.1.23)</a> and <a href="#x1-45r24">(5.1.24)</a> would both equal zero:

				\begin {equation*} 
					\begin {array}{rclr}
						c_1 \Big( \overset{W(x)=0}{\overbrace{  y_1 \displaystyle\frac{dy_2}{dx} - \displaystyle\frac{dy_1}{dx} y_2 }} \Big) &=& k_0 \displaystyle\frac{dy_2}{dx} - k_1 y_2 &\quad\quad\quad (5.1.23) \\ \\
						c_2 \Big( \underset{W(x)=0}{\underbrace{ y_1 \displaystyle\frac{dy_2}{dx} - \displaystyle\frac{dy_1}{dx} y_2 }} \Big) &=& k_1 y_1 - k_0 \displaystyle\frac{dy_1}{dx} &\quad\quad\quad (5.1.24) \\
					\end {array} 
				\end {equation*}

				Which means that \( k_0 \) and \( k_1 \) would have to satisfy this system of equations

				\begin {equation*} 
					\quad \quad 
					\left\{
					\begin {array}{rclr}
						 0 &=& k_0 \displaystyle\frac{dy_2}{dx} - k_1 y_2 &\quad\quad\quad  \\ \\
						 0 &=& k_1 y_1 - k_0 \displaystyle\frac{dy_1}{dx} &\quad\quad\quad  \\
					\end {array} 
					\right.
				\end {equation*}

				which is only true in rare situations. This motivates us to
				consider conditions on \(y_1\) and \(y_2\) where the Wronskian is nonzero <a href="#x1-46r25">(5.1.25)</a>

			</div>				
			
		</details>	


		
		
		<details id="4e05e5cd-134c-4ff0-99a9-6003c3a6964d">
			<summary>Abel's Formula</summary>
	
			<div class="indent" id="6b78dd65-8eaa-42d4-8882-d86b97cb722f">
		
			</div>

			<div class="indent" id="02f4b4d9-c339-42d2-80ce-85c102723bee">

			</div>

			<div class="theorem" id="002fe4c0-e999-4687-9729-bba038a3cc7f">
				<div>
					<span class="head">Theorem&nbsp;5.1.4</span>
					<a id="x1-484"></a>
					Suppose \(p(x)\) and \(q(x)\) are continuous on \((a,b),\) let \(y_1\) and
					\(y_2\) be solutions of <a href="#x1-49r27">(5.1.27)</a> on \((a,b)\)
					
					<a id="x1-49r27"></a>
					\begin {equation} \label {eq:5.1.27}
						\frac{d^2y}{dx^2}+p(x)\frac{dy}{dx}+q(x)y=0 
					\end {equation}
					
					Define the <span class="vocab">Wronskian</span> determinant of \( \{y_1,y_2\} \) as 
					
					<a id="x1-50r28"></a>
					\begin {equation} \label {eq:5.1.28}
						W(x) = 
						\det
						\left(
							\begin{array}{cc}
								y_1   & y_2   \\
								\displaystyle \frac{dy_1}{dx}   & \displaystyle \frac{dy_2}{dx}  
							\end{array}
						\right)
						=y_1 \displaystyle\frac{dy_2}{dx} - \displaystyle\frac{dy_1}{dx} y_2 
					\end {equation}
					
					Let \(x_0\) be any point in \((a,b).\) Then <span class="vocab">Abel's formula</span> is 
					
					<a id="x1-51r29"></a>
					\begin {equation} \label{eq:5.1.29} 
						W(x)=W(x_0)  \cdot  e^{-\displaystyle \int ^x_{x_0}p(t)\, dt}
					\end {equation}
					
					And therefore either \(W(x)\) has no zeros in \((a,b)\) or \(W(x) \equiv 0\) on \((a,b)\)
				</div>
			</div>

			<div id="5f80fb36-94ba-401c-9379-d9b5b08de13b">
				<div class="reading-exercise" >
					Differentiate <a href="#x1-50r28">(5.1.28)</a> using the product rule
				</div> 
				<div>
					Differentiating <a href="#x1-50r28">(5.1.28)</a> yields
					<a id="x1-52r30"></a>
					\begin {equation} \label {eq:5.1.30}
						\frac{dW}{dx}= y_1\frac{d^2y_2}{dx^2}-\frac{d^2y_1}{dx^2}y_2
					\end {equation}
				</div>
				
				<div class="reading-exercise" >
					Substitute \(y_1\) and \(y_2\) <a href="#x1-49r27">(5.1.27)</a> to obtain expressions for \( \frac{d^2y_1}{dx^2} \) and \( \frac{d^2y_2}{dx^2} \)
				</div> 

				<div>
					Substituting the expressions for \( \frac{d^2y_1}{dx^2} \) and \( \frac{d^2y_2}{dx^2} \) into <a href="#x1-52r30">(5.1.30)</a> and rearranging terms yields:

					\begin{align*}
						\frac{dW}{dx} 
						= & 
							y_1 \Big ( -p\frac{dy_2}{dx} - qy_2 \Big) 
							- y_2 \Big( - p\frac{dy_1}{dx} - qy_1 \Big) \\ 
						= & 
							-y_1 \Big ( p\frac{dy_2}{dx} + qy_2 \Big) 
							+ y_2 \Big(  p\frac{dy_1}{dx} + qy_1 \Big) \\
						= & 
							- p y_1 \frac{dy_2}{dx} 
							- q y_1 y_2
							+ p \frac{dy_1}{dx} y_2 
							+ q y_1 y_2 \\	
						= & 
							- p y_1 \frac{dy_2}{dx} 
							+ p \frac{dy_1}{dx} y_2
							- \underset{0}{\underbrace{q y_1 y_2 + q y_1 y_2}} \\	
						= & 
							- p \Big( y_1 \frac{dy_2}{dx} 
								- \frac{dy_1}{dx} y_2 \Big) \\
						= & 
							- p 
							\cdot
							\det
							\left(
								\begin{array}{cc}
									y_1   & y_2   \\
									\displaystyle \frac{dy_1}{dx}   & \displaystyle \frac{dy_2}{dx}  
								\end{array}
							\right) \\
						= &
							- p \, W \\	
					\end{align*}
					
					And we can rearrange \( \displaystyle \frac{dW}{dx} = - p \, W \) to produce the first order linear differential equation
					
					\[ 
						\frac{dW}{dx} + p(x) \, W = 0
					\]

					Which means that \(y = W\) is the solution of the IVP 
					
					\[ 
						\frac{dy}{dx}+p(x)y=0
						\quad y(x_0)=W(x_0)
					\] 

					

					We immediately can identify the trivial solution \(y \equiv 0 \)
				</div>

				<div class="reading-exercise" >
					Verify \( y\equiv 0 \) is a solution of \( \frac{dy}{dx}+p(x)y=0 \)
				</div> 

				<div>
					Now assuming \( y \not \equiv 0 \) we can separate the variables
	
					\begin{align*}
						\frac{dy}{dx} + p(x)\,y = & 0 \\
						\frac{dy}{dx}  = & -p(x)\,y \\
						\frac{1}{y} \cdot \frac{dy}{dx}  = & -p(x) \\
						\int \frac{1}{y} dy  = & - \int p(x) dx \\		
						\ln \left| y \right|  = & c - \int^x_{x_0} p(t) dt \\	
						\left| y \right| = & c e^{- \displaystyle \int^x_{x_0} p(t) dt} \\					
					\end{align*}

					And since it is an exponential function and never changes sign we can drop the absolute value bars so we have

					\begin{align*}
						y = & c e^{- \displaystyle \int^x_{x_0} p(t) dt} \\					
					\end{align*}
				</div>
				<div class="reading-exercise" >
					Verify \( y = c e^{- \int^x_{x_0} p(t) dt}\) is a solution of \( \frac{dy}{dx}+p(x)y=0 \)
				</div> 	
				<div>
					And now evaluating the solution at the initial condition \( y(x_0)=W(x_0) \) we obtain

					\begin{align*}
						\underset{W(x_0)}{\underbrace{y(x_0)}} = & c e^{- \displaystyle \underset{0}{\underbrace{\int^{x_0}_{x_0} p(t) dt}}} \\ \\
						W(x_0) = & c \, e^0
						\Longrightarrow
						c = W(x_0)					
					\end{align*}

					Substituting back the value for the constant \( c = W(x_0) \) we obtain the particular solution to the IVP
	
					\begin{align*}
						y = & W(x_0)  \cdot  e^{- \displaystyle \int^x_{x_0} p(t) dt} \\					
					\end{align*}
				</div>
				<div class="reading-exercise" >
					Verify \( y = W(x_0)  \cdot  e^{- \int^x_{x_0} p(t) dt}\) is a solution of \( \frac{dy}{dx}+p(x)y=0 \)
				</div> 	
				<div>	
					And since we know \(y=W\) we can rewrite our solution as <span class="vocab">Abel's formula</span> <a href="#x1-51r29">(5.1.29)</a>: 

					\begin{align*}
						W(x) = & W(x_0)  \cdot  e^{- \displaystyle \int^x_{x_0} p(t) dt} \\					
					\end{align*}

					Then we can analyze it fairly straightforwardly since if \(W(x_0)=0\) 

					\begin{align*}
						W(x) = & \overset{0}{\overbrace{W(x_0)}}  \cdot  e^{- \displaystyle \int^x_{x_0} p(t) dt} 
						\equiv 0\\					
					\end{align*}

					Meaning that if the Wronskian evaluated at the initial \(x\) value \(x_0\) evaluates to zero, then that means that the Wronskian itself must be identically zero. And if the Wronskian evaluated at the initial \(x\) value \(x_0\) does not evaluate to zero \(W(x_0)\ne 0\) then \(W(x)\) can never be zero since <span class="vocab">Abel's formula</span> shows us it is an exponential function which is never zero. 
				</div>	

		
			</div>
			
			<br>
			<br>
			<br>
			<br>

			<!--Example 5-->
			<div class="example" id="x1-535" style="--ex-number: 5">
				<div>
					Verify <span class="vocab">Abel's formula</span> <a href="#x1-51r29">(5.1.29)</a> for Examples&nbsp;<a href="#x1-61">5.1.1</a>,
					<a href="#x1-142">5.1.2</a>, and
					<a href="#x1-233">5.1.3</a>
				</div>

				<ol>
					<li>
						Example <a href="#x1-61">5.1.1</a> 
						\[	
							\frac{d^2y}{dx^2}-y=0 
							\quad \quad 
							y_1=e^x 
							\quad y_2=e^{-x}
						\]
					</li>
					<li>
						Example <a href="#x1-142">5.1.2</a> 
						\[
							\frac{d^2y}{dx^2}+\omega ^2y=0
							\quad \quad
							y_1=\cos \omega x
							\quad y_2=\sin \omega x
						\]
					</li>
					<li>
						Example <a href="#x1-233">5.1.3</a>
						\[
							x^2\frac{d^2y}{dx^2}+x\frac{dy}{dx}-4y=0
							\quad \quad 
							y_1=x^2
							\quad y_2=1/x^2
						\]
					</li>
				</ol>

				<details class="solution" id="c1a1da63-10d2-4a75-ae84-af58cd259569">
					<summary>Solution</summary>

					<ol>
						<li>
							<div>
								In Example <a href="#x1-61">5.1.1</a> we already identified \( p(x) \equiv 0 \) using linear standard form <a href="#x1-2r1">(5.1.1)</a> then <span class="vocab">Abel's formula</span> <a href="#x1-51r29">(5.1.29)</a> will evaluate to a constant:

								\[
									W(x)= \underset{\text{constant}}{\underbrace{W(x_0)}}  \cdot e^{- \underset{\text{constant}}{\underbrace{\displaystyle\int ^x_{x_0}\overset{p(t)}{\overbrace{0}}\, dt }} }
								\]	

								So we will verify Abel’s formula by showing that the Wronskian <a href="#x1-50r28">(5.1.28)</a> is also constant.
									\begin {equation*}
										W(x) = 
										\det
										\left(
											\begin{array}{cc}
												y_1   & y_2   \\
												\displaystyle \frac{dy_1}{dx}   & \displaystyle \frac{dy_2}{dx}  
											\end{array}
										\right)
										=y_1 \displaystyle\frac{dy_2}{dx} - \displaystyle\frac{dy_1}{dx} y_2 
									\end {equation*}
							</div>
							<div>
								From Example <a href="#x1-61">5.1.1</a> we have the solutions and their derivatives
								
								\begin{align*}
									\begin{array}{rccrcc}
											y_1 &=& e^x  			\quad & \quad  y_2 &=& e^{-x}  \\
											\displaystyle \frac{y_1}{dx} &=& e^x  \quad & \quad  \displaystyle \frac{y_2}{dx} &=& -e^{-x}  
									\end{array}
								\end{align*}

								Substituting them in to the Wronskian <a href="#x1-50r28">(5.1.28)</a> we obtain:

								\begin {align*}
									W(x) = 
									\det
									\left(
										\begin{array}{cc}
											e^x  & e^{-x}  \\
											e^x  & -e^{-x}  
										\end{array}
									\right)
										= & e^x(-e^{-x}) - e^xe^{-x} \\
										= & -e^{x-x } - e^{x-x} \\
										= & -2e^{0}
										= -2 
								\end{align*}

								\(W(x) = -2 \) <em>is</em> constant so Abel’s formula is verified

							</div>	
							
						</li>
						<li>
							<div>
								In Example <a href="#x1-142">5.1.2</a> we already identified \( p(x) \equiv 0 \) using linear standard form <a href="#x1-2r1">(5.1.1)</a> then <span class="vocab">Abel's formula</span> <a href="#x1-51r29">(5.1.29)</a> will evaluate to a constant:

								\[
									W(x)= \underset{\text{constant}}{\underbrace{W(x_0)}}  \cdot  e^{- \underset{\text{constant}}{\underbrace{\displaystyle\int ^x_{x_0}\overset{p(t)}{\overbrace{0}}\, dt }} }
								\]	

								So we will verify Abel’s formula by showing that the Wronskian <a href="#x1-50r28">(5.1.28)</a> is also constant.
									\begin {equation*}
										W(x) = 
										\det
										\left(
											\begin{array}{cc}
												y_1   & y_2   \\
												\displaystyle \frac{dy_1}{dx}   & \displaystyle \frac{dy_2}{dx}  
											\end{array}
										\right)
										=y_1 \displaystyle\frac{dy_2}{dx} - \displaystyle\frac{dy_1}{dx} y_2 
									\end {equation*}
							</div>
							<div>
								In Example <a href="#x1-142">5.1.2</a> we have the solutions and their derivatives
								
								\begin{align*}
									\begin{array}{rccrcc}
											y_1 &=& \cos\omega x  			\quad & \quad  y_2 &=& \sin\omega x  \\
											\displaystyle \frac{y_1}{dx} &=& -\omega\sin\omega x  \quad & \quad  \displaystyle \frac{y_2}{dx} &=& \omega\cos\omega x  
									\end{array}
								\end{align*}

								Substituting them in to the Wronskian <a href="#x1-50r28">(5.1.28)</a> we obtain:

								
								\begin {align*}
									W(x) = & 
									\det 
									\left(  \\[15pt]
												\begin{array}{cc}
													\cos\omega x  & \sin\omega x  \\
													-\omega\sin\omega x & \omega\cos\omega x  
												\end{array}
											\right) \\
										= & \big(\cos\omega x\big)\big(\omega\cos\omega x\big) - \big(\sin\omega x\big)\big(-\omega\sin\omega x\big) \\
										= & \omega\cos^2\omega x + \omega\sin^2\omega x \\
										= & \omega \big( \underset{1}{\underbrace{\cos^2\omega x + \sin^2\omega x}} \big)
										= \omega 
								\end{align*}

								\(W(x) = \omega \) <em>is</em> constant so Abel’s formula is verified

							</div>	
							
						</li>	
						<li>
							<div>
								In Example <a href="#x1-233">5.1.3</a> we already identified \( p(x) = 1/x \) using linear standard form <a href="#x1-2r1">(5.1.1)</a> and since \(p(x)\) is undefined at \( x = 0 \) 
								we only consider solutions on intervals that don't contain zero so \((-\infty ,0)\) and \((0,\infty )\). Then <span class="vocab">Abel's formula</span> <a href="#x1-51r29">(5.1.29)</a> yields:

								\begin{align*}
									W(x)
									= & W(x_0) \cdot e^{- \displaystyle\int ^x_{x_0}\overset{p(t)}{\overbrace{\frac{1}{t}} \, dt } } \\
									= & W(x_0) \cdot e^{- \displaystyle \left.\\[10pt] \ln \left| t \right| \, \right|_{x_0}^x } \\
									= & W(x_0) \cdot e^{-  \displaystyle \big( \ln \left| x \right| - \ln \left| x_0 \right| \big) } \\
									= & W(x_0) \cdot e^{ \displaystyle \big(  \ln \left| x_0 \right|  - \ln \left| x \right| \big) } \\
									= & W(x_0) \cdot e^{ \displaystyle  \ln \left| x_0/x \right| } \\
									= & W(x_0) \cdot \left| \frac{x_0}{x} \right| \\	
									= & W(x_0) \cdot \frac{x_0}{x} \\						
								\end{align*}	

								Since both \( x \) and \(x_0\) must be in either  \((-\infty ,0)\) or \((0,\infty )\) so they are either both positive or both negative so the ratio \( {x_0}/{x} \) is always positive.

								We will now show that the Wronskian <a href="#x1-50r28">(5.1.28)</a> also equals \(W(x_0) \cdot \displaystyle \frac{x_0}{x}  \)
									\begin {equation*}
										W(x) = 
										\det
										\left(
											\begin{array}{cc}
												y_1   & y_2   \\
												\displaystyle \frac{dy_1}{dx}   & \displaystyle \frac{dy_2}{dx}  
											\end{array}
										\right)
										=y_1 \displaystyle\frac{dy_2}{dx} - \displaystyle\frac{dy_1}{dx} y_2 
									\end {equation*}
							</div>
							<div>
								In Example <a href="#x1-233">5.1.3</a> we have the solutions and their derivatives
								
								\begin{align*}
									\begin{array}{rccrcc}
											y_1 &=& x^2  			\quad & \quad  y_2 &=& \displaystyle \frac{1}{x^2}  \\
											\displaystyle \frac{y_1}{dx} &=& 2x  \quad & \quad  \displaystyle \frac{y_2}{dx} &=& - \displaystyle \frac{2}{x^3}  
									\end{array}
								\end{align*}

								Substituting them in to the Wronskian <a href="#x1-50r28">(5.1.28)</a> we obtain:

								\begin {align}
									W(x) = 
									\det
									\left(  \\[15pt]
												\begin{array}{rr}
													\, x^2 & \displaystyle \frac{1}{x^2} \,  \nonumber \\
													\, 2x 	& - \displaystyle \frac{2}{x^3} \,  
												\end{array}
											\right)
										= & x^2 \left( - \frac{2}{x^3} \right) - 2x \left( \frac{1}{x^2} \right) \nonumber \\
										= & - \frac{2}{x}  - \frac{2}{x}  
										= - \frac{4}{x} 
								\end{align}

								Now according to Abel’s formula

								\begin {align*}
									\underbrace{W(x)} \overset{?}{=} & \underbrace{W(x_0)} \cdot \frac{x_0}{x}  \\
									- \frac{4}{x}  \overset{?}{=} & - \frac{4}{x_0} \cdot \frac{x_0}{x} \\
									- \frac{4}{x}  = & - \frac{4}{x} \quad \checkmark
								\end{align*}


								As we have \( W(x) = W(x_0) \cdot \displaystyle \frac{x_0}{x} \) Abel’s formula <em>is</em> verified
							</div>	
						</li>	
					</ol>
				</details>
			</div>
			<br>

			<!--Example 6-->
			<div class="example" id="x1-5356" style="--ex-number: 6">
				<div>
					Assuming only one solution \(y_1\) for a second order equation is given, use <span class="vocab-repeat">Abel's formula</span> to write and solve a first order equation for \(y_2\) for Examples&nbsp;<a href="#x1-61">5.1.1</a>,
					<a href="#x1-142">5.1.2</a>, and
					<a href="#x1-233">5.1.3</a>
				</div>

				<ol>
					<li>
						Example <a href="#x1-61">5.1.1</a> 
						\[	
							\frac{d^2y}{dx^2}-y=0 
							\quad \quad 
							y_1=e^x 
						\]
					</li>
					<li>
						Example <a href="#x1-142">5.1.2</a> 
						\[
							\frac{d^2y}{dx^2}+\omega ^2y=0
							\quad \quad
							y_1=\cos \omega x
						\]
					</li>
					<li>
						Example <a href="#x1-233">5.1.3</a>
						\[
							x^2\frac{d^2y}{dx^2}+x\frac{dy}{dx}-4y=0
							\quad \quad 
							y_1=x^2
						\]
					</li>
				</ol>

				<details class="solution" id="c1a1da63-10d2-4a75-ae84-af58cd259569">
					<summary>Solution</summary>

					<ol>
						<li>
							<div>
								We have been given one solution \(y_1=e^x \) and we will use it to find a second solution \(y_2\) using <span class="vocab">Abel's formula</span> <a href="#x1-51r29">(5.1.29)</a>

									\begin{align*}
										W(x) = & W(x_0) \cdot e^{- {\displaystyle\int ^x_{x_0} p(t) \, dt } }
									\end{align*}	
									
								The left side of Abel's formula is the <span class="vocab">Wronskian</span> determinant of a fundamental set of solutions \( \{y_1,y_2\} \) and is given by Equation <a href="#x1-50r28">(5.1.28)</a>

								\begin {equation*}
									W(x) = 
									\det
									\left(
										\begin{array}{cc}
											y_1   & y_2   \\
											\displaystyle \frac{dy_1}{dx}   & \displaystyle \frac{dy_2}{dx}  
										\end{array}
									\right)
									=y_1 \displaystyle\frac{dy_2}{dx} - \displaystyle\frac{dy_1}{dx} y_2 
								\end {equation*}	

								So we can rewrite Abel's formula as

								\begin{align*}
									y_1 \displaystyle\frac{dy_2}{dx} - \displaystyle\frac{dy_1}{dx} y_2 = & \, \underset{c}{\underbrace{W(x_0)}} \cdot e^{- {\displaystyle\int ^x_{x_0} p(t) \, dt } }
								\end{align*}
								
								And the right side of Abel's formula contains a factor \(W(x_0)\) which is the <span class="vocab-repeat">Wronskian</span> determinant evaluated at some initial value \(x_0\) and as such we will consider it some arbitrary constant \(W(x_0) = c\)
								so we can rewrite Abel's formula as

								\begin{align*}
									y_1 \displaystyle\frac{dy_2}{dx} - \displaystyle\frac{dy_1}{dx} y_2 = & \, c \, \cdot \, e^{- {\displaystyle\int ^x_{x_0} p(t) \, dt } }
								\end{align*}

								Differentiating the given \(y_1=e^x \) we obtain \(dy_1/dx=e^x \) so the left side becomes

								\begin{align*}
									y_1 \displaystyle\frac{dy_2}{dx} - \displaystyle\frac{dy_1}{dx} y_2 
									=
									e^x \displaystyle\frac{dy_2}{dx} - e^x y_2 
								\end{align*}

								From linear standard form <a href="#x1-2r1">(5.1.1)</a> we know \( p(t)\equiv 0 \) so the right side becomes an arbitrary constant

								\begin{align*}
									c \, \cdot \, e^{- {\displaystyle\int ^x_{x_0} \, 0  \, dt } }
									=
									c \, \cdot \, e^{- k }
									= c
								\end{align*}
								
								Finally we obtain a first order equation for \(y_2\)

								\begin{align*}
									e^x \displaystyle\frac{dy_2}{dx} - e^x y_2 = c
								\end{align*}

								Which we simplify by writing in linear standard form fixme (2.1.1)
								 taking the arbitary constant \(c=1\) and just writing \(y_2\) as \(y\)
								
							</div>
							<div>	

								

								Which we will substitute the given \(y_1=e^x \) and its derivative \(dy_1/dx=e^x \) into while leaving \(y_2\) and \(dy_2/dx \) so we can write a first order linear equation to solve for \(y_2\)
							</div>

							<div>		
								We start by identifying \( p(x) \) 
								
							</div>
							<div>			
								Now substitute \(y_1=e^x \) and \(p(x)\) into Abel's formula <a href="#x1-51r29">(5.1.29)</a> and simplifying the constant \( W(x_0) = c \) and throughout to \(c=1\)				
								\begin{align*}
									W[y_1,y_2](x) = & \underbrace{W(x_0)}  \cdot e^{- {\displaystyle\int ^x_{x_0} p(t) \, dt } } \\
									\det
									\left(
										\begin{array}{cc}
											y_1   & y_2   \\
											\displaystyle \frac{dy_1}{dx}   & \displaystyle \frac{dy_2}{dx}  
										\end{array}
									\right)
									= & \, c  \cdot e^{- {\displaystyle\int ^x_{x_0} p(t) \, dt } } \\
									\det
									\left(
										\begin{array}{cc}
											e^x   & y_2   \\
											e^x  & \displaystyle \frac{dy_2}{dx}  
										\end{array}
									\right)
									= & \, e^{- {\displaystyle\int ^x_{x_0} 0 \, dt } } 
									= \, e^{- c } = c \\		
									e^x \frac{dy_2}{dx} - e^x y_2
									& = 1 \\
									\frac{dy_2}{dx} - y_2
									& = e^{-x}								\\																	
								\end{align*}


								Thus we have obtained a differential equation to find \(y_2\) which is a first order linear equation so we can solve the same way we did in chapter 2 by first solving the complementary equation
								\begin{align*}
									\frac{dy}{dx} - y = & 0 \\
									\frac{dy}{dx} = & y \\
									\int \frac{1}{y} dy = & \int dx \\
									\ln y = & \, x \\
									y = & \, e^x \\
								\end{align*}

								And then using variation of parameters to seek solutions of the form \(y_2=u e^x\)
								\begin{align*}
									\frac{dy_2}{dx} - y_2 = & \,  e^{-x} \\	
								\end{align*}
							</div>
							<div>	

								So we will verify Abel’s formula by showing that the Wronskian <a href="#x1-50r28">(5.1.28)</a> is also constant.
									\begin {equation*}
										W(x) = 
										\left|
											\begin{array}{cc}
												y_1   & y_2   \\
												\displaystyle \frac{dy_1}{dx}   & \displaystyle \frac{dy_2}{dx}  
											\end{array}
										\right|
										=y_1 \displaystyle\frac{dy_2}{dx} - \displaystyle\frac{dy_1}{dx} y_2 
									\end {equation*}
							</div>
							<div>
								From Example <a href="#x1-61">5.1.1</a> we have the solutions and their derivatives
								
								\begin{align*}
									\begin{array}{rccrcc}
											y_1 &=& e^x  			\quad & \quad  y_2 &=& e^{-x}  \\
											\displaystyle \frac{y_1}{dx} &=& e^x  \quad & \quad  \displaystyle \frac{y_2}{dx} &=& -e^{-x}  
									\end{array}
								\end{align*}

								Substituting them in to the Wronskian <a href="#x1-50r28">(5.1.28)</a> we obtain:

								\begin {align*}
									W(x) = 
									\left|
												\begin{array}{cc}
													e^x  & e^{-x}  \\
													e^x  & -e^{-x}  
												\end{array}
											\right|
										= & e^x(-e^{-x}) - e^xe^{-x} \\
										= & -e^{x-x } - e^{x-x} \\
										= & -2e^{0}
										= -2 
								\end{align*}

								\(W(x) = -2 \) <em>is</em> constant so Abel’s formula is verified

							</div>	
							
						</li>
						<li>
							<div>
								In Example <a href="#x1-142">5.1.2</a> we already identified \( p(x) \equiv 0 \) using linear standard form <a href="#x1-2r1">(5.1.1)</a> then <span class="vocab">Abel's formula</span> <a href="#x1-51r29">(5.1.29)</a> will evaluate to a constant:

								\[
									W(x)= \underset{\text{constant}}{\underbrace{W(x_0)}}  \cdot  e^{- \underset{\text{constant}}{\underbrace{\displaystyle\int ^x_{x_0}\overset{p(t)}{\overbrace{0}}\, dt }} }
								\]	

								So we will verify Abel’s formula by showing that the Wronskian <a href="#x1-50r28">(5.1.28)</a> is also constant.
									\begin {equation*}
										W(x) = 
										\left|
													\begin{array}{cc}
														y_1   & y_2   \\
														\displaystyle \frac{dy_1}{dx}   & \displaystyle \frac{dy_2}{dx}  
													\end{array}
												\right|
										=y_1 \displaystyle\frac{dy_2}{dx} - \displaystyle\frac{dy_1}{dx} y_2 
									\end {equation*}
							</div>
							<div>
								In Example <a href="#x1-142">5.1.2</a> we have the solutions and their derivatives
								
								\begin{align*}
									\begin{array}{rccrcc}
											y_1 &=& \cos\omega x  			\quad & \quad  y_2 &=& \sin\omega x  \\
											\displaystyle \frac{y_1}{dx} &=& -\omega\sin\omega x  \quad & \quad  \displaystyle \frac{y_2}{dx} &=& \omega\cos\omega x  
									\end{array}
								\end{align*}

								Substituting them in to the Wronskian <a href="#x1-50r28">(5.1.28)</a> we obtain:

								
								\begin {align*}
									W(x) = &  
									\left|  \\[15pt]
												\begin{array}{cc}
													\cos\omega x  & \sin\omega x  \\
													-\omega\sin\omega x & \omega\cos\omega x  
												\end{array}
											\right| \\
										= & \big(\cos\omega x\big)\big(\omega\cos\omega x\big) - \big(\sin\omega x\big)\big(-\omega\sin\omega x\big) \\
										= & \omega\cos^2\omega x + \omega\sin^2\omega x \\
										= & \omega \big( \underset{1}{\underbrace{\cos^2\omega x + \sin^2\omega x}} \big)
										= \omega 
								\end{align*}

								\(W(x) = \omega \) <em>is</em> constant so Abel’s formula is verified

							</div>	
							
						</li>	
						<li>
							<div>
								In Example <a href="#x1-233">5.1.3</a> we already identified \( p(x) = 1/x \) using linear standard form <a href="#x1-2r1">(5.1.1)</a> and since \(p(x)\) is undefined at \( x = 0 \) 
								we only consider solutions on intervals that don't contain zero so \((-\infty ,0)\) and \((0,\infty )\). Then <span class="vocab">Abel's formula</span> <a href="#x1-51r29">(5.1.29)</a> yields:

								\begin{align*}
									W(x)
									= & W(x_0) \cdot e^{- \displaystyle\int ^x_{x_0}\overset{p(t)}{\overbrace{\frac{1}{t}} \, dt } } \\
									= & W(x_0) \cdot e^{- \displaystyle \left.\\[10pt] \ln \left| t \right| \, \right|_{x_0}^x } \\
									= & W(x_0) \cdot e^{-  \displaystyle \big( \ln \left| x \right| - \ln \left| x_0 \right| \big) } \\
									= & W(x_0) \cdot e^{ \displaystyle \big(  \ln \left| x_0 \right|  - \ln \left| x \right| \big) } \\
									= & W(x_0) \cdot e^{ \displaystyle  \ln \left| x_0/x \right| } \\
									= & W(x_0) \cdot \left| \frac{x_0}{x} \right| \\	
									= & W(x_0) \cdot \frac{x_0}{x} \\						
								\end{align*}	

								Since both \( x \) and \(x_0\) must be in either  \((-\infty ,0)\) or \((0,\infty )\) so they are either both positive or both negative so the ratio \( {x_0}/{x} \) is always positive.

								We will now show that the Wronskian <a href="#x1-50r28">(5.1.28)</a> also equals \(W(x_0) \cdot \displaystyle \frac{x_0}{x}  \)
									\begin {equation*}
										W(x) = 
										\left|
													\begin{array}{cc}
														y_1   & y_2   \\
														\displaystyle \frac{dy_1}{dx}   & \displaystyle \frac{dy_2}{dx}  
													\end{array}
												\right|
										=y_1 \displaystyle\frac{dy_2}{dx} - \displaystyle\frac{dy_1}{dx} y_2 
									\end {equation*}
							</div>
							<div>
								In Example <a href="#x1-233">5.1.3</a> we have the solutions and their derivatives
								
								\begin{align*}
									\begin{array}{rccrcc}
											y_1 &=& x^2  			\quad & \quad  y_2 &=& \displaystyle \frac{1}{x^2}  \\
											\displaystyle \frac{y_1}{dx} &=& 2x  \quad & \quad  \displaystyle \frac{y_2}{dx} &=& - \displaystyle \frac{2}{x^3}  
									\end{array}
								\end{align*}

								Substituting them in to the Wronskian <a href="#x1-50r28">(5.1.28)</a> we obtain:

								\begin {align}
									W(x) = 
									\left|  \\[15pt]
												\begin{array}{rr}
													\, x^2 & \displaystyle \frac{1}{x^2} \,  \nonumber \\
													\, 2x 	& - \displaystyle \frac{2}{x^3} \,  
												\end{array}
											\right|
										= & x^2 \left( - \frac{2}{x^3} \right) - 2x \left( \frac{1}{x^2} \right) \nonumber \\
										= & - \frac{2}{x}  - \frac{2}{x}  
										= - \frac{4}{x} 
								\end{align}

								Now according to Abel’s formula

								\begin {align*}
									\underbrace{W(x)} \overset{?}{=} & \underbrace{W(x_0)} \cdot \frac{x_0}{x}  \\
									- \frac{4}{x}  \overset{?}{=} & - \frac{4}{x_0} \cdot \frac{x_0}{x} \\
									- \frac{4}{x}  = & - \frac{4}{x} \quad \checkmark
								\end{align*}


								As we have \( W(x) = W(x_0) \cdot \displaystyle \frac{x_0}{x} \) Abel’s formula <em>is</em> verified
							</div>	
						</li>	
					</ol>
				</details>
			</div>

		</details>		


		<!--Summary-->
		<details id="a726b580-934a-4a2f-9371-d9cc658d903dsa7">
			<summary>Summary</summary>			

			<div class="indent" id="2cbe302f-9b7a-4c07-8655-953619194ae3">
				The next theorem will enable us to complete the proof of Theorem&nbsp;<a href="#x1-363">5.1.3</a>.
			</div>
			<div class="theorem" id="f4373cd3-83a3-4a23-92b8-3b0d45bc671d">
				<div>
					<span class="head">Theorem&nbsp;5.1.5</span>
					<a id="x1-585"></a>
					Suppose \(p(x)\) and \(q(x)\) are continuous on an open interval \((a,b),\)
					let \(y_1\) and \(y_2\) be solutions of <a href="#x1-59r32">(5.1.32)</a> on \((a,b)\)
					
					<a id="x1-59r32"></a>
					\begin {equation} \label{eq:5.1.32} 
						\frac{d^2y}{dx^2}+p(x)\frac{dy}{dx}+q(x)y=0 
					\end {equation}
					
					Then \(y_1\) and \(y_2\) are linearly independent on \((a,b)\) if and only if the Wronskian of \( \{ y_1, y_2 \} \) has no zeros on \((a,b)\)
				</div>
			</div>

			<div class="indent" id="b30c4d43-b381-4f06-8eb4-64bd5165478c">
				We can now complete the proof of Theorem&nbsp;<a href="#x1-363">5.1.3</a>.
				From Theorem&nbsp;<a href="#x1-585">5.1.5</a>, two solutions \(y_1\) and \(y_2\) of <a href="#x1-59r32">(5.1.32)</a> are linearly independent on \((a,b)\) if and only if \(W\) has no zeros
				on \((a,b)\). From Theorem&nbsp;<a href="#x1-484">5.1.4</a>
				and the motivating comments preceding it, \(\{y_1,y_2\}\) is a fundamental
				set of solutions of <a href="#x1-59r32">(5.1.32)</a> if and only if \(W\) has no zeros on \((a,b)\). Therefore
				\(\{y_1,y_2\}\) is a fundamental set for <a href="#x1-59r32">(5.1.32)</a> on \((a,b)\) if and only if \(\{y_1,y_2\}\) is linearly independent on
				\((a,b)\).
			</div>
			<div class="indent" id="b64cceb3-c7b2-4c88-9a30-02102c439421">
				The next theorem summarizes the relationships among the concepts discussed
				in this section.
			</div>
			<div class="theorem" id="80b7bc20-8e17-45e7-b6c1-c556422a8596">
				<div>
					<a id="x1-616"></a>
					<span class="head">Theorem&nbsp;5.1.6</span>
					Suppose \(p(x)\) and \(q(x)\) are continuous on an open interval \((a,b)\) and
					let \(y_1\) and \(y_2\) be solutions of <a href="#x1-62r34">(5.1.34)</a> on \((a,b)\)
					
					<a id="x1-62r34"></a>
					\begin {equation} \label{eq:5.1.34} 
						\frac{d^2y}{dx^2}+p(x)\frac{dy}{dx}+q(x)y=0 
					\end {equation}
					
					Then the following statements are equivalent and they are either all true or all false
				</div>

				<ol>
					<li>
						The general solution of <a href="#x1-62r34">(5.1.34)</a> on \((a,b)\) is \(y=c_1y_1+c_2y_2\)
					</li>
					<li>
						\(\{y_1,y_2\}\) is a fundamental set of solutions of <a href="#x1-62r34">(5.1.34)</a> on \((a,b)\)
					</li>
					<li>
						\(\{y_1,y_2\}\) is linearly independent on \((a,b)\)
					</li>
					<li>
						The Wronskian of \(\{y_1,y_2\}\) is nonzero at some point in \((a,b)\)
					</li>
					<li>
						The Wronskian of \(\{y_1,y_2\}\) is nonzero at all points in \((a,b)\)
					</li>
				</ol>
			</div>
			
			<br>
			<br>
			<br>


			<div class="indent" id="70490c73-9675-460b-8646-d6e6a8ffa3f9">
				We can apply this theorem to an equation written as 
				
				\[
					P_0(x)\frac{d^2y}{dx^2}+P_1(x)\frac{dy}{dx}+P_2(x)y=0 
				\] 
				
				on an interval \((a,b)\) where \(P_0(x)\), \(P_1(x)\), and \(P_2(x)\) are continuous and \(P_0(x)\) has no zeros.
			</div>

			<div class="theorem" id="a852152e-d82c-4bef-adde-46a42ddd7f8b">
				<div>
					<span class="head">Theorem&nbsp;5.1.7</span>
					<a id="x1-687"></a>
					Suppose \(c\) is in \((a,b)\) and \(\alpha \) and \(\beta \) are real
					numbers, not both zero. Under the assumptions of Theorem&nbsp;<a href="#x1-687">5.1.7</a>,
					suppose \(y_{1}\) and \(y_{2}\) are solutions of <a href="#x1-62r34">(5.1.34)</a> such that 
					
					<a id="x1-69r35"></a>
					\begin {equation} \label {eq:5.1.35} 
						\alpha y_{1}(c)+\beta
						\left.\frac{dy_1}{dx}\right|_c = 0
						\quad \text { and } \quad 
						\alpha y_{2}(c)+\beta \left.\frac{dy_2}{dx}\right|_c = 0
					\end {equation}
					
					Then \(\{y_{1},y_{2}\}\) isn’t linearly independent on \((a,b)\)
				</div>
			</div>

			<div  class="removed" id="a771876c-07bd-4fe9-a702-07f159c263b9">
				<span class="ptmb8t-">Proof</span>
				Since \(\alpha \) and \(\beta \) are not both zero, <a href="#x1-69r35">(5.1.35)</a> implies that 
				
				\[
					\left.\\[25pt]
						\left|
							\begin {array}{ccccccc}
								\, y_{1} &amp; \displaystyle \frac{dy_1}{dx} \,  \\
								\, y_{2} &amp; \displaystyle \frac{dy_2}{dx} 
							\end {array}
						\right|
					\right|_{c} = 0
					\quad \text { so }\quad 
					\left.\\[20pt]
						\left |
							\begin {array}{cccccc} 
								\, y_{1} &amp;y_{2} \,\\
								\, \frac{dy_1}{dx} &amp; \frac{dy_2}{dx}\,  
							\end {array}
						\right|
					\right|_{c}=0 
				\] 
				
				
				and Theorem&nbsp;<a href="#x1-616">5.1.6</a> implies the stated conclusion.
			</div>
		</details>

		

		<nav class = "ex">
			<a href = "sec5.1.ex.html">5.1 Exercises</a>
		</nav>

		<nav class = "book">
			<a href="sec4.0.html">Back (Chapter 4)</a>

			<a href="sec5.0.html">Chapter 5</a>

			<a href="sec5.2.book.html">Next (5.2)</a>
		</div>
	</body>
</html>
