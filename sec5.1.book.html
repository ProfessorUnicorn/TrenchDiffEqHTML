<!doctype html>
<html lang="en-US" xml:lang="en-US">
	<head>
		<title>5.1 Homogeneous Linear Equations</title>
		<meta charset="utf-8" />
		<meta content="TeX4ht (https://tug.org/tex4ht/)" name="generator" />
		<meta content="width=device-width,initial-scale=1" name="viewport" />
		<link rel="stylesheet" type="text/css" href="combined.css" />
		<meta content="Trench_DiffEQ_Book.tex" name="src" />
		<script>
			window.MathJax = {
				tex: {
					packages: { "[+]": ["tagformat"] },
					tags: "ams",
					tagformat: { tag: (tag) => "(5.1." + tag + ")" },
				},
			};
		</script>
		<script
			async="async"
			id="MathJax-script"
			src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js"
			type="text/javascript"
		></script>
		<style type="text/css">
			:root {
				--section-number: "5.1";
			}
		</style>
	</head>
	<body>
		<div id="3ac7df10-b122-4149-aa5b-3a959ab6b43b" class="section-title">
			5.1 Homogeneous Linear Equations
		</div>
		<details id="54f23eb8-7365-45fc-ad4f-e14b44acf04d">
			<summary>Homogeneous Linear Equations</summary>
		
			<div id="d645654c-c987-442a-b5eb-abf6685bc698">
				A second order differential equation is said to be
				<span class="vocab">linear</span>
				if it can be written as \begin {equation} \label {eq:5.1.1}
				\frac{d^2y}{dx^2}+p(x)\frac{dy}{dx}+q(x)y=f(x)\end {equation}
				<a id="x1-2r1"></a>
				We call the function \(f\) on the right a
				<span class="vocab">forcing function</span>,
				since in physical applications it’s often related to a force acting on
				some system modeled by the differential equation. We say that <a href="#x1-2r1">(5.1.1)</a> is
				<span class="vocab">homogeneous</span>
				if \(f\equiv 0\) or
				<span class="vocab">nonhomogeneous</span>
				if \(f\not \equiv 0\). Since these definitions are like the corresponding
				definitions in Section&nbsp;2.1 for the linear first order equation \begin
				{equation} \label {eq:5.1.2} \frac{dy}{dx}+p(x)y=f(x)\end {equation}
				<a id="x1-3r2"></a>
				it’s natural to expect similarities between methods of solving <a href="#x1-2r1">(5.1.1)</a> and <a href="#x1-3r2">(5.1.2)</a>. However, solving <a href="#x1-2r1">(5.1.1)</a> is more difficult than solving <a href="#x1-3r2">(5.1.2)</a>. For example, while Theorem&nbsp;
				<span class="ptmb8t-">2.1.1</span>
				gives a formula for the general solution of <a href="#x1-3r2">(5.1.2)</a> in the case where \(f\equiv 0\) and Theorem&nbsp;
				<span class="ptmb8t-">2.1.2</span>
				gives a formula for the case where \(f\not \equiv 0\), there are no
				formulas for the general solution of <a href="#x1-2r1">(5.1.1)</a> in either case. Therefore we must be content to solve linear second
				order equations of special forms.
			</div>
			<div class="indent" id="43af8b13-63ec-4e0d-8870-9e2002119e3b">
				In Section&nbsp;2.1 we considered the homogeneous equation \(\frac{dy}{dx}+p(x)y=0\)
				first, and then used a nontrivial solution of this equation to find the
				general solution of the nonhomogeneous equation \(\frac{dy}{dx}+p(x)y=f(x)\).
				Although the progression from the homogeneous to the nonhomogeneous case
				isn’t that simple for the linear second order equation, it’s still
				necessary to solve the homogeneous equation \begin {equation} \label
				{eq:5.1.3} \frac{d^2y}{dx^2} +p(x)\frac{dy}{dx}+q(x)y=0 \end {equation}
				<a id="x1-4r3"></a>
				in order to solve the nonhomogeneous equation <a href="#x1-2r1">(5.1.1)</a>. This section is devoted to <a href="#x1-4r3">(5.1.3)</a>.
			</div>
			<div class="indent" id="7c0a5f10-8ad8-4a30-89db-22ede2de14ed">
				The next theorem gives sufficient conditions for existence and uniqueness
				of solutions of initial value problems for <a href="#x1-4r3">(5.1.3)</a>. We omit the proof.
			</div>

			<div class="theorem" id="198cbe42-7032-4493-b0d4-8482ad90bf09">
				<div>
					<span class="head">Theorem&nbsp;5.1.1</span>

					<a id="x1-51"></a>
					Suppose \(p\) and \(q\) are continuous on an open interval \((a,b),\)
					let \(x_0\) be any point in \((a,b),\) and let \(k_0\) and \(k_1\) be
					arbitrary real numbers\(.\) Then the initial value problem \[
					\frac{d^2y}{dx^2}+p(x)\frac{dy}{dx}+q(x)y=0,\ y(x_0)=k_0,\ \left.\frac{dy}{dx}\right|_{x_0}=k_1 \] has a unique solution
					on \((a,b).\)
				</div>
			</div>

			<div class="indent" id="0ef53e7f-77f6-4789-91ff-c2df4f6e9198">
				Since \(y\equiv 0\) is obviously a solution of <a href="#x1-4r3">(5.1.3)</a> we call it the
				<span class="vocab">trivial</span> solution. Any other solution is
				<span class="vocab">nontrivial</span>. Under the assumptions of Theorem&nbsp;<a href="#x1-51">5.1.1</a>,
				the only solution of the initial value problem \[ \frac{d^2y}{dx^2}+p(x)\frac{dy}{dx}+q(x)y=0,\
				y(x_0)=0,\ \left.\frac{dy}{dx}\right|_{x_0}=0 \] on \((a,b)\) is the trivial solution
				(Exercise&nbsp;
				<span class="ptmb8t-">24</span>
				).
			</div>
			<div class="indent" id="ba67d92f-5f10-4f37-8a5f-f2c2e5a91673">
				The next three examples illustrate concepts that we’ll develop later in
				this section. You shouldn’t be concerned with how to
				<em>find</em>
				the given solutions of the equations in these examples. This will be
				explained in later sections.
			</div>

			<div class="example" id="x1-61" style="--ex-number: 1">
				<div>
					The coefficients of \(\frac{dy}{dx}\) and \(y\) in \begin {equation} \label
					{eq:5.1.4} \frac{d^2y}{dx^2}-y=0 \end {equation}
					<a id="x1-7r4"></a>
					are the constant functions \(p\equiv 0\) and \(q\equiv -1\), which are
					continuous on \((-\infty ,\infty )\). Therefore Theorem&nbsp;<a href="#x1-51">5.1.1</a>
					implies that every initial value problem for <a href="#x1-7r4">(5.1.4)</a> has a unique solution on \((-\infty ,\infty )\).
				</div>

				<ol>
					<li>
						Verify that \(y_1=e^x\) and \(y_2=e^{-x}\) are solutions of <a href="#x1-7r4">(5.1.4)</a> on \((-\infty ,\infty )\).
					</li>
					<li>
						Verify that if \(c_1\) and \(c_2\) are arbitrary constants,
						\(y=c_1e^x+c_2e^{-x}\) is a solution of <a href="#x1-7r4">(5.1.4)</a> on \((-\infty ,\infty )\).
					</li>
					<li>
						<a id="x1-11r5"></a>
						Solve the initial value problem \begin {equation} \label {eq:5.1.5}
						\frac{d^2y}{dx^2}-y=0,\quad y(0)=1,\quad \left.\frac{dy}{dx}\right|_{0}=3\end {equation}
					</lil>
				</ol>

				<details class="solution" id="d3cafea5-d9fe-4743-bd86-24a64d90c53f">
					<summary>Solution</summary>
					<ol>
						<li>
							If \(y_1=e^x\) then \(\frac{dy_1}{dx}=e^x\) and \(\frac{d^2y_1}{dx^2}=e^x=y_1\), so
							\(\frac{d^2y_1}{dx^2}-y_1=0\). If \(y_2=e^{-x}\), then \(\frac{dy_2}{dx}=-e^{-x}\) and
							\(\frac{d^2y_2}{dx^2}=e^{-x}=y_2\), so \(\frac{d^2y_2}{dx^2}-y_2=0\).
						</li>
						<li>
							If \begin {equation} \label {eq:5.1.6} y=c_1e^x+c_2e^{-x} \end {equation}
							<a id="x1-12r6"></a>
							then \begin {equation} \label {eq:5.1.7} \frac{dy}{dx}=c_1e^x-c_2e^{-x} \end
							{equation}
							<a id="x1-13r7"></a>
							and \[ \frac{d^2y}{dx^2}=c_1e^x+c_2e^{-x}\] so
							<div class="eqnarray" id="f1f9c23f-edb9-48a6-ada6-86e2ccc6d9ce">
								\begin {eqnarray*} \frac{d^2y}{dx^2}-y&amp;=&amp;(c_1e^x+c_2e^{-x})-(c_1e^x+c_2e^{-x})\\
								&amp;=&amp;c_1(e^x-e^x)+c_2(e^{-x}-e^{-x})=0 \end {eqnarray*}
							</div>
							for all \(x\). Therefore \(y=c_1e^x+c_2e^{-x}\) is a solution of <a href="#x1-7r4">(5.1.4)</a> on \((-\infty ,\infty )\).
						</li>
						<li>
							We can solve <a href="#x1-11r5">(5.1.5)</a> by choosing \(c_1\) and \(c_2\) in <a href="#x1-12r6">(5.1.6)</a> so that \(y(0)=1\) and \(\left.\frac{dy}{dx}\right|_{0}=3\). Setting \(x=0\) in <a href="#x1-12r6">(5.1.6)</a> and <a href="#x1-13r7">(5.1.7)</a> shows that this is equivalent to
							<div class="eqnarray" id="b38bafeb-f5c8-4ced-a795-568f39bc43c3">
								\begin {eqnarray*} c_1+c_2&amp;=&amp;1\\ c_1-c_2&amp;=&amp;3\end
								{eqnarray*}
							</div>
							Solving these equations yields \(c_1=2\) and \(c_2=-1\). Therefore
							\(y=2e^x-e^{-x}\) is the unique solution of <a href="#x1-11r5">(5.1.5)</a> on \((-\infty ,\infty )\).
						</li>
					</ol>
				</details>
			</div>

			<div class="example" id="x1-142" style="--ex-number: 2">
				<div>
					<a id="x1-"></a>
					Let \(\omega \) be a positive constant. The coefficients of \(\frac{dy}{dx}\) and
					\(y\) in \begin {equation} \label {eq:5.1.8} \frac{d^2y}{dx^2}+\omega ^2y=0 \end
					{equation}
					<a id="x1-15r8"></a>
					are the constant functions \(p\equiv 0\) and \(q\equiv \omega ^2\),
					which are continuous on \((-\infty ,\infty )\). Therefore Theorem&nbsp;<a href="#x1-51">5.1.1</a>
					implies that every initial value problem for <a href="#x1-15r8">(5.1.8)</a> has a unique solution on \((-\infty ,\infty )\).
				</div>
				<ol>
					<li>
						Verify that \(y_1=\cos \omega x\) and \(y_2=\sin \omega x\) are
						solutions of <a href="#x1-15r8">(5.1.8)</a> on \((-\infty ,\infty )\).
					</li>
					<li>
						Verify that if \(c_1\) and \(c_2\) are arbitrary constants then
						\(y=c_1\cos \omega x+c_2\sin \omega x\) is a solution of <a href="#x1-15r8">(5.1.8)</a> on \((-\infty ,\infty )\).
					</li>
					<li>
						<a id="x1-19r9"></a>
						Solve the initial value problem \begin {equation} \label {eq:5.1.9}
						\frac{d^2y}{dx^2}+\omega ^2y=0,\quad y(0)=1,\quad \left.\frac{dy}{dx}\right|_{0}=3\end {equation}
					</li>
				</ol>
				<details class="solution" id="857aa373-5a74-4453-bd1c-ec8c4b2b8989">
					<summary>Solution</summary>
					<ol>
						<li>
							If \(y_1=\cos \omega x\) then \(\frac{dy_1}{dx}=-\omega \sin \omega x\) and
							\(\frac{d^2y_1}{dx^2}=-\omega ^2\cos \omega x=-\omega ^2y_1\), so \(\frac{d^2y_1}{dx^2}+\omega
							^2y_1=0\). If \(y_2=\sin \omega x\) then, \(\frac{dy_2}{dx}=\omega \cos \omega x\)
							and \(\frac{d^2y_2}{dx^2}=-\omega ^2\sin \omega x=-\omega ^2y_2\), so \(\frac{d^2y_2}{dx^2}+\omega
							^2y_2=0\).
						</li>
						<li>
							If \begin {equation} \label {eq:5.1.10} y=c_1\cos \omega x+c_2\sin \omega
							x \end {equation}
							<a id="x1-20r10"></a>
							then \begin {equation} \label {eq:5.1.11} \frac{dy}{dx}=\omega (-c_1\sin \omega
							x+c_2\cos \omega x) \end {equation}
							<a id="x1-21r11"></a>
							and \[ \frac{d^2y}{dx^2}=-\omega ^2(c_1\cos \omega x+c_2\sin \omega x)\] so

							<div class="eqnarray" id="be85aab1-a8f8-4807-b9ff-f4d1b20d0c8d">
								\begin {eqnarray*} \frac{d^2y}{dx^2}+\omega ^2y&amp;=&amp; -\omega ^2(c_1\cos \omega
								x+c_2\sin \omega x) +\omega ^2(c_1\cos \omega x+c_2\sin \omega x)\\
								&amp;=&amp;c_1\omega ^2(-\cos \omega x+\cos \omega x)+ c_2\omega ^2(-\sin
								\omega x+\sin \omega x)=0 \end {eqnarray*}
							</div>

							for all \(x\). Therefore \(y=c_1\cos \omega x+c_2\sin \omega x\) is a
							solution of <a href="#x1-15r8">(5.1.8)</a> on \((-\infty ,\infty )\).
						</li>
						<li>
							To solve <a href="#x1-19r9">(5.1.9)</a>, we must choosing \(c_1\) and \(c_2\) in <a href="#x1-20r10">(5.1.10)</a> so that \(y(0)=1\) and \(\left.\frac{dy}{dx}\right|_{0}=3\). Setting \(x=0\) in <a href="#x1-20r10">(5.1.10)</a> and <a href="#x1-21r11">(5.1.11)</a> shows that \(c_1=1\) and \(c_2=3/\omega \). Therefore \[ y=\cos \omega
							x+{3\over \omega }\sin \omega x \] is the unique solution of <a href="#x1-19r9">(5.1.9)</a> on \((-\infty ,\infty )\).
						</li>
					</ol>
				</details>
			</div>


			<div class="indent" id="994bd946-38c4-47d2-9892-c54f712d2e08">
				Theorem&nbsp;<a href="#x1-51">5.1.1</a>
				implies that if \(k_0\) and \(k_1\) are arbitrary real numbers then the
				initial value problem \begin {equation} \label {eq:5.1.12}
				P_0(x)\frac{d^2y}{dx^2}+P_1(x)\frac{dy}{dx}+P_2(x)y=0,\quad y(x_0)=k_0,\quad \left.\frac{dy}{dx}\right|_{x_0}=k_1 \end
				{equation}
				<a id="x1-22r12"></a>
				has a unique solution on an interval \((a,b)\) that contains \(x_0\),
				provided that \(P_0\), \(P_1\), and \(P_2\) are continuous and \(P_0\) has
				no zeros on \((a,b)\). To see this, we rewrite the differential equation
				in <a href="#x1-22r12">(5.1.12)</a> as \[ \frac{d^2y}{dx^2}+{P_1(x)\over P_0(x)}\frac{dy}{dx}+{P_2(x)\over P_0(x)}y=0 \] and apply
				Theorem&nbsp;<a href="#x1-51">5.1.1</a>
				with \(p=P_1/P_0\) and \(q=P_2/P_0\).
			</div>
			
			<div class="example" id="x1-233" style="--ex-number: 3">
				<div>
					<a id="x1-24r13"></a>
					The equation \begin {equation} \label {eq:5.1.13} x^2\frac{d^2y}{dx^2}+x\frac{dy}{dx}-4y=0 \end {equation}
					has the form of the differential equation in <a href="#x1-22r12">(5.1.12)</a>, with \(P_0(x)=x^2\), \(P_1(x)=x\), and \(P_2(x)=-4\), which are are
					all continuous on \((-\infty ,\infty )\). However, since \(P(0)=0\) we
					must consider solutions of <a href="#x1-24r13">(5.1.13)</a> on \((-\infty ,0)\) and \((0,\infty )\). Since \(P_0\) has no zeros on
					these intervals, Theorem&nbsp;<a href="#x1-51">5.1.1</a>
					implies that the initial value problem \[ x^2\frac{d^2y}{dx^2}+x\frac{dy}{dx}-4y=0,\quad
					y(x_0)=k_0,\quad \left.\frac{dy}{dx}\right|_{x_0}=k_1 \] has a unique solution on \((0,\infty )\)
					if \(x_0&gt;0\), or on \((-\infty ,0)\) if \(x_0&lt;0\).
				</div>

				<ol>
					<li>
						Verify that \(y_1=x^2\) is a solution of <a href="#x1-24r13">(5.1.13)</a> on \((-\infty ,\infty )\) and \(y_2=1/x^2\) is a solution of <a href="#x1-24r13">(5.1.13)</a> on \((-\infty ,0)\) and \((0,\infty )\).
					</li>
					<li>
						Verify that if \(c_1\) and \(c_2\) are any constants then
						\(y=c_1x^2+c_2/x^2\) is a solution of <a href="#x1-24r13">(5.1.13)</a> on \((-\infty ,0)\) and \((0,\infty )\).
					</li>
					<li>
						<a id="x1-28r14"></a>
						Solve the initial value problem \begin {equation} \label {eq:5.1.14}
						x^2\frac{d^2y}{dx^2}+x\frac{dy}{dx}-4y=0,\quad y(1)=2,\quad \left.\frac{dy}{dx}\right|_{1}=0\end {equation}
					</li>
					<li>
						<a id="x1-30r15"></a>
						Solve the initial value problem \begin {equation} \label {eq:5.1.15}
						x^2\frac{d^2y}{dx^2}+x\frac{dy}{dx}-4y=0,\quad y(-1)=2,\quad \left.\frac{dy}{dx}\right|_{-1}=0\end {equation}
					</li>
				</ol>

				<details class="solution" id="ac9b3a00-5b58-4d10-889b-eefc35d85843">
					<ol>
						<li>
							If \(y_1=x^2\) then \(\frac{dy_1}{dx}=2x\) and \(\frac{d^2y_1}{dx^2}=2\), so \[
							x^2\frac{d^2y_1}{dx^2}+x\frac{dy_1}{dx}-4y_1=x^2(2)+x(2x)-4x^2=0 \] for \(x\) in \((-\infty ,\infty
							)\). If \(y_2=1/x^2\), then \(\frac{dy_2}{dx}=-2/x^3\) and \(\frac{d^2y_2}{dx^2}=6/x^4\), so \[
							x^2\frac{d^2y_2}{dx^2}+x\frac{dy_2}{dx}-4y_2=x^2\left (6\over x^4\right )-x\left (2\over x^3\right
							)-{4\over x^2}=0 \] for \(x\) in \((-\infty ,0)\) or \((0,\infty )\).
						</li>
						<li>
							If \begin {equation} \label {eq:5.1.16} y=c_1x^2+{c_2\over x^2} \end
							{equation}
							<a id="x1-31r16"></a>
							then \begin {equation} \label {eq:5.1.17} \frac{dy}{dx}=2c_1x-{2c_2\over x^3} \end
							{equation}
							<a id="x1-32r17"></a>
							and \[ \frac{d^2y}{dx^2}=2c_1+{6c_2\over x^4}\] so
							<div class="eqnarray" id="515bebcb-41e3-4f1b-9a8b-530263b6404d">
								\begin {eqnarray*} x^2\frac{d^2y}{dx^2}+x\frac{dy}{dx}-4y&amp;=&amp;x^2\displaystyle {\left
								(2c_1+{6c_2\over x^4}\right )} +x\displaystyle {\left (2c_1x-{2c_2\over
								x^3}\right )} -4\displaystyle {\left (c_1x^2+{c_2\over x^2}\right )}\\
								&amp;=&amp;c_1(2x^2+2x^2-4x^2) +c_2\displaystyle {\left ({6\over
								x^2}-{2\over x^2}-{4\over x^2}\right )}\\ &amp;=&amp;c_1\cdot 0+c_2\cdot
								0=0 \end {eqnarray*}
							</div>
							for \(x\) in \((-\infty ,0)\) or \((0,\infty )\).
						</li>
						<li>
							To solve <a href="#x1-28r14">(5.1.14)</a>, we choose \(c_1\) and \(c_2\) in <a href="#x1-31r16">(5.1.16)</a> so that \(y(1)=2\) and \(\left.\frac{dy}{dx}\right|_{1}=0\). Setting \(x=1\) in <a href="#x1-31r16">(5.1.16)</a> and <a href="#x1-32r17">(5.1.17)</a> shows that this is equivalent to
							<div class="eqnarray" id="2d7b04e5-9680-4376-bcb3-3906d9aded2d">
								\begin {eqnarray*} \phantom {2}c_1+\phantom {2}c_2&amp;=&amp;2\\
								2c_1-2c_2&amp;=&amp;0\end {eqnarray*}
							</div>
							Solving these equations yields \(c_1=1\) and \(c_2=1\). Therefore
							\(y=x^2+1/x^2\) is the unique solution of <a href="#x1-28r14">(5.1.14)</a> on \((0,\infty )\).
						</li>
						<li>
							We can solve <a href="#x1-30r15">(5.1.15)</a> by choosing \(c_1\) and \(c_2\) in <a href="#x1-31r16">(5.1.16)</a> so that \(y(-1)=2\) and \(\left.\frac{dy}{dx}\right|_{-1}=0\). Setting \(x=-1\) in <a href="#x1-31r16">(5.1.16)</a> and <a href="#x1-32r17">(5.1.17)</a> shows that this is equivalent to
							<div class="eqnarray" id="c38a4121-915d-4873-810d-a598d1eef339">
								\begin {eqnarray*} \phantom {-2}c_1+\phantom {2}c_2&amp;=&amp;2\\
								-2c_1+2c_2&amp;=&amp;0\end {eqnarray*}
							</div>
							Solving these equations yields \(c_1=1\) and \(c_2=1\). Therefore
							\(y=x^2+1/x^2\) is the unique solution of <a href="#x1-30r15">(5.1.15)</a> on \((-\infty ,0)\).
						</li>
					</ol>
				</details>
			</div>

			<div class="indent" id="467c5185-9cf7-47c1-9bbe-0f1bce3be4a1">
				Although the <em>formulas</em>
				for the solutions of <a href="#x1-28r14">(5.1.14)</a> and <a href="#x1-30r15">(5.1.15)</a> are both \(y=x^2+1/x^2\), you should not conclude that these two initial
				value problems have the same solution. Remember that a solution of an
				initial value problem is defined on an interval that contains the initial
				point; therefore, the solution of <a href="#x1-28r14">(5.1.14)</a> is \(y=x^2+1/x^2\) on the interval \((0,\infty )\), which contains the
				initial point \(x_0=1\), while the solution of <a href="#x1-30r15">(5.1.15)</a> is \(y=x^2+1/x^2\) on the interval \((-\infty ,0)\), which contains the
				initial point \(x_0=-1\).
			</div>
		</details>

		<details id="935e85a1-5159-413b-a779-b952361b8b4c">
			<summary>The General Solution of a Homogeneous Linear Second Order Equation</summary>

			<div id="dcf91a05-365b-4abb-8756-c0df04e1205f">
				If \(y_1\) and \(y_2\) are defined on an interval \((a,b)\) and \(c_1\)
				and \(c_2\) are constants, then \[ y=c_1y_1+c_2y_2 \] is a
				<span class="vocab">linear combination</span>
				of \(y_1\) and \(y_2\). For example, \(y=2\cos x+7 \sin x\) is a linear
				combination of \(y_1= \cos x\) and \(y_2=\sin x\), with \(c_1=2\) and
				\(c_2=7\).
			</div>
			<div class="indent" id="96180d4c-a867-498f-89dc-41a8784432c0">
				The next theorem states a fact that we’ve already verified in
				Examples&nbsp;<a href="#x1-61">5.1.1</a>,
				<a href="#x1-142">5.1.2</a>, and
				<a href="#x1-233">5.1.3</a>.
			</div>

			<div class="theorem" id="c5e939e5-3da8-4282-941e-dfa60fa89893">
				<div>
					<span class="head">Theorem&nbsp;5.1.2</span>
					<a id="x1-332"></a>
					If \(y_1\) and \(y_2\) are solutions of the homogeneous equation \begin
					{equation} \label {eq:5.1.18} \frac{d^2y}{dx^2}+p(x)\frac{dy}{dx}+q(x)y=0 \end {equation}
					<a id="x1-34r18"></a>
					on \((a,b),\) then any linear combination \begin {equation} \label
					{eq:5.1.19} y=c_1y_1+c_2y_2 \end {equation}
					<a id="x1-35r19"></a>
					of \(y_1\) and \(y_2\) is also a solution of \(\eqref {eq:5.1.18}\) on
					\((a,b).\)
				</div>
			</div>

			<div id="e2a0c89d-a971-4b15-ad83-b46ccd25fbf8">
				<span class="ptmb8t-">Proof</span>
				If \[ y=c_1y_1+c_2y_2 \] then \[ \frac{dy}{dx}=c_1\frac{dy_1}{dx}+c_2\frac{dy_2}{dx}\quad \mbox{ and } \quad  \frac{d^2y}{dx^2}=c_1\frac{d^2y_1}{dx^2}+c_2\frac{d^2y_2}{dx^2}\] Therefore

				<div class="eqnarray" id="992c794b-e810-46b5-bb67-967851986160">
					\begin {eqnarray*}
					\frac{d^2y}{dx^2}+p(x)\frac{dy}{dx}+q(x)y&amp;=&amp;(c_1\frac{d^2y_1}{dx^2}+c_2\frac{d^2y_2}{dx^2})+p(x)(c_1\frac{dy_1}{dx}+c_2\frac{dy_2}{dx})
					+q(x)(c_1y_1+c_2y_2)\\ &amp;=&amp;c_1\left (\frac{d^2y_1}{dx^2}+p(x)\frac{dy_1}{dx}+q(x)y_1\right )
					+c_2\left (\frac{d^2y_2}{dx^2}+p(x)\frac{dy_2}{dx}+q(x)y_2\right )\\ &amp;=&amp;c_1\cdot 0+c_2\cdot
					0=0\end {eqnarray*}
				</div>

				since \(y_1\) and \(y_2\) are solutions of <a href="#x1-34r18">(5.1.18)</a>.
			</div>

			<div class="indent" id="cd388051-210f-4db7-9cdd-f4d32479734e">
				We say that \(\{y_1,y_2\}\) is a
				<span class="vocab">fundamental set of solutions of</span>
				\(\eqref {eq:5.1.18}\) on \((a,b)\) if every solution of <a href="#x1-34r18">(5.1.18)</a> on \((a,b)\) can be written as a linear combination of \(y_1\) and
				\(y_2\) as in <a href="#x1-35r19">(5.1.19)</a>. In this case we say that <a href="#x1-35r19">(5.1.19)</a> is
				<span class="vocab">general solution of</span>
				\(\eqref {eq:5.1.18}\) on \((a,b)\).
			</div>
		</details>

		<details id="4e05e5cd-134c-4ff0-99a9-6003c3a6964d">
			<summary>Linear Independence</summary>
		
			<div id="0e43a534-0f0b-40d8-93f0-3c8c894215a3">
				We need a way to determine whether a given set \(\{y_1,y_2\}\) of
				solutions of <a href="#x1-34r18">(5.1.18)</a> is a fundamental set. The next definition will enable us to state
				necessary and sufficient conditions for this.
			</div>
			<div class="indent" id="8886d612-039f-40e8-908a-2153ca66e3a8">
				We say that two functions \(y_1\) and \(y_2\) defined on an interval
				\((a,b)\) are
				<span class="vocab">linearly independent</span>
				on \((a,b)\) if neither is a constant multiple of the other on \((a,b)\).
				(In particular, this means that neither can be the trivial solution of <a href="#x1-34r18">(5.1.18)</a>, since, for example, if \(y_1\equiv 0\) we could write \(y_1=0y_2\).)
				We’ll also say that the set \(\{y_1,y_2\}\) is
				<span class="vocab">linearly independent</span>
				on \((a,b)\).
			</div>
			<div class="theorem" id="a5aa20a7-255d-4d9e-978c-b2d9d72509d2">
				<div>
					<span class="head">Theorem&nbsp;5.1.3</span>
					<a id="x1-363"></a>
					Suppose \(p\) and \(q\) are continuous on \((a,b).\) Then a set
					\(\{y_1,y_2\}\) of solutions of \begin {equation} \label {eq:5.1.20}
					\frac{d^2y}{dx^2}+p(x)\frac{dy}{dx}+q(x)y=0 \end {equation}
					<a id="x1-37r20"></a>
					on \((a,b)\) is a fundamental set if and only if \(\{y_1,y_2\}\) is
					linearly independent on \((a,b).\)
				</div>
			</div>

			<div class="indent" id="40fabbc2-5ca7-4e81-a280-3706257d9809">
				We’ll present the proof of Theorem&nbsp;<a href="#x1-363">5.1.3</a>
				in steps worth regarding as theorems in their own right. However, let’s
				first interpret Theorem&nbsp;<a href="#x1-363">5.1.3</a>
				in terms of Examples&nbsp;<a href="#x1-61">5.1.1</a>,
				<a href="#x1-142">5.1.2</a>, and
				<a href="#x1-233">5.1.3</a>.
			</div>

			<div class="example" id="x1-384" style="--ex-number: 4">
				<ol>
					<li>
						Since \(e^x/e^{-x}=e^{2x}\) is nonconstant, Theorem&nbsp;<a href="#x1-363">5.1.3</a>
						implies that \(y=c_1e^x+c_2e^{-x}\) is the general solution of
						\(\frac{d^2y}{dx^2}-y=0\) on \((-\infty ,\infty )\).
					</li>
					<li>
						Since \(\cos \omega x/\sin \omega x=\cot \omega x\) is nonconstant,
						Theorem&nbsp;<a href="#x1-363">5.1.3</a>
						implies that \(y=c_1\cos \omega x+c_2\sin \omega x\) is the general
						solution of \(\frac{d^2y}{dx^2}+\omega ^2y=0\) on \((-\infty ,\infty )\).
					</li>
					<li>
						Since \(x^2/x^{-2}=x^4\) is nonconstant, Theorem&nbsp;<a href="#x1-363">5.1.3</a>
						implies that \(y=c_1x^2+c_2/x^2\) is the general solution of
						\(x^2\frac{d^2y}{dx^2}+x\frac{dy}{dx}-4y=0\) on \((-\infty ,0)\) and \((0,\infty )\).
					</li>
				</ol>
			</div>
		</details>
		
		<details id="a726b580-934a-4a2f-9371-d9cc658d9037">
			<summary>The Wronskian and Abel’s Formula</summary>
		
			<div id="b0ad295a-f305-47cc-ab47-ac09b8cee963">
				To motivate a result that we need in order to prove Theorem&nbsp;<a href="#x1-363">5.1.3</a>, let’s see what is required to prove that \(\{y_1,y_2\}\) is a
				fundamental set of solutions of <a href="#x1-37r20">(5.1.20)</a> on \((a,b)\). Let \(x_0\) be an arbitrary point in \((a,b)\), and
				suppose \(y\) is an arbitrary solution of <a href="#x1-37r20">(5.1.20)</a> on \((a,b)\). Then \(y\) is the unique solution of the initial value
				problem \begin {equation} \label {eq:5.1.21} \frac{d^2y}{dx^2}+p(x)\frac{dy}{dx}+q(x)y=0,\quad
				y(x_0)=k_0,\quad \left.\frac{dy}{dx}\right|_{x_0}=k_1; \end {equation}
				<a id="x1-42r21"></a>
				that is, \(k_0\) and \(k_1\) are the numbers obtained by evaluating \(y\)
				and \(\frac{dy}{dx}\) at \(x_0\). Moreover, \(k_0\) and \(k_1\) can be any real
				numbers, since Theorem&nbsp;<a href="#x1-51">5.1.1</a>
				implies that <a href="#x1-42r21">(5.1.21)</a> has a solution no matter how \(k_0\) and \(k_1\) are chosen. Therefore
				\(\{y_1,y_2\}\) is a fundamental set of solutions of <a href="#x1-37r20">(5.1.20)</a> on \((a,b)\) if and only if it’s possible to write the solution of an
				arbitrary initial value problem <a href="#x1-42r21">(5.1.21)</a> as \(y=c_1y_1+c_2y_2\). This is equivalent to requiring that the system
				\begin {equation} \label {eq:5.1.22} \begin {array}{rcl}
				c_1y_1(x_0)+c_2y_2(x_0)&amp;=&amp;k_0\\
				c_1\left.\frac{dy_1}{dx}\right|_{x_0}+c_2\left.\frac{dy_2}{dx}\right|_{x_0}&amp;=&amp;k_1 \end {array} \end {equation}
				<a id="x1-43r22"></a>
				has a solution \((c_1,c_2)\) for every choice of \((k_0,k_1)\). Let’s try
				to solve <a href="#x1-43r22">(5.1.22)</a>.
			</div>
			<div class="indent" id="2286a18e-657e-4f73-8364-8c37502e96b5">
				Multiplying the first equation in <a href="#x1-43r22">(5.1.22)</a> by \(\left.\frac{dy_2}{dx}\right|_{x_0}\) and the second by \(y_2(x_0)\) yields
			</div>
			<div class="eqnarray" id="491f663f-16b0-4c97-9bb9-a330a67e9990">
				\begin {eqnarray*} c_1y_1(x_0)\left.\frac{dy_2}{dx}\right|_{x_0}+c_2y_2(x_0)\left.\frac{dy_2}{dx}\right|_{x_0}&amp;=&amp;
				\left.\frac{dy_2}{dx}\right|_{x_0}k_0\\ c_1\left.\frac{dy_1}{dx}\right|_{x_0}y_2(x_0)+c_2\left.\frac{dy_2}{dx}\right|_{x_0}y_2(x_0)&amp;=&amp;
				y_2(x_0)k_1\end {eqnarray*}
			</div>
			<div class="indent" id="8ee3123e-3632-49c5-a613-55e5485a51f9">
				and subtracting the second equation here from the first yields \begin
				{equation} \label {eq:5.1.23} \left
				(y_1(x_0)\left.\frac{dy_2}{dx}\right|_{x_0}-\left.\frac{dy_1}{dx}\right|_{x_0}y_2(x_0)\right )c_1=
				\left.\frac{dy_2}{dx}\right|_{x_0}k_0-y_2(x_0)k_1\end {equation}
				<a id="x1-44r23"></a>
				Multiplying the first equation in <a href="#x1-43r22">(5.1.22)</a> by \(\left.\frac{dy_1}{dx}\right|_{x_0}\) and the second by \(y_1(x_0)\) yields
			</div>
			<div class="eqnarray" id="1b60beb3-675b-4f69-9c24-ff298a19080e">
				\begin {eqnarray*} c_1y_1(x_0)\left.\frac{dy_1}{dx}\right|_{x_0}+c_2y_2(x_0)\left.\frac{dy_1}{dx}\right|_{x_0}&amp;=&amp;
				\left.\frac{dy_1}{dx}\right|_{x_0}k_0\\ c_1\left.\frac{dy_1}{dx}\right|_{x_0}y_1(x_0)+c_2\left.\frac{dy_2}{dx}\right|_{x_0}y_1(x_0)&amp;=&amp;
				y_1(x_0)k_1\end {eqnarray*}
			</div>
			<div class="indent" id="6b78dd65-8eaa-42d4-8882-d86b97cb722f">
				and subtracting the first equation here from the second yields \begin
				{equation} \label {eq:5.1.24} \left
				(y_1(x_0)\left.\frac{dy_2}{dx}\right|_{x_0}-\left.\frac{dy_1}{dx}\right|_{x_0}y_2(x_0)\right )c_2=
				y_1(x_0)k_1-\left.\frac{dy_1}{dx}\right|_{x_0}k_0\end {equation}
				<a id="x1-45r24"></a>
				If \[ y_1(x_0)\left.\frac{dy_2}{dx}\right|_{x_0}-\left.\frac{dy_1}{dx}\right|_{x_0}y_2(x_0)=0\] it’s impossible to satisfy <a href="#x1-44r23">(5.1.23)</a> and <a href="#x1-45r24">(5.1.24)</a> (and therefore <a href="#x1-43r22">(5.1.22)</a>) unless \(k_0\) and \(k_1\) happen to satisfy
			</div>
			<div class="eqnarray" id="f45d570f-cb84-43eb-8d8e-85250a0cdeab">
				\begin {eqnarray*} y_1(x_0)k_1-\left.\frac{dy_1}{dx}\right|_{x_0}k_0&amp;=&amp;0\\
				\left.\frac{dy_2}{dx}\right|_{x_0}k_0-y_2(x_0)k_1&amp;=&amp;0\end {eqnarray*}
			</div>
			<div class="indent" id="02f4b4d9-c339-42d2-80ce-85c102723bee">
				On the other hand, if \begin {equation} \label {eq:5.1.25}
				y_1(x_0)\left.\frac{dy_2}{dx}\right|_{x_0}-\left.\frac{dy_1}{dx}\right|_{x_0}y_2(x_0)\ne 0 \end {equation}
				<a id="x1-46r25"></a>
				we can divide <a href="#x1-44r23">(5.1.23)</a> and <a href="#x1-45r24">(5.1.24)</a> through by the quantity on the left to obtain \begin {equation} \label
				{eq:5.1.26} \begin {array}{rcl} c_1&amp;=&amp;\displaystyle
				{\left.\frac{dy_2}{dx}\right|_{x_0}k_0-y_2(x_0)k_1\over y_1(x_0)\left.\frac{dy_2}{dx}\right|_{x_0}-\left.\frac{dy_1}{dx}\right|_{x_0}y_2(x_0)}\\
				c_2&amp;=&amp;\displaystyle {y_1(x_0)k_1-\left.\frac{dy_1}{dx}\right|_{x_0}k_0\over
				y_1(x_0)\left.\frac{dy_2}{dx}\right|_{x_0}-\left.\frac{dy_1}{dx}\right|_{x_0}y_2(x_0)}\end {array} \end {equation}
				<a id="x1-47r26"></a>
				no matter how \(k_0\) and \(k_1\) are chosen. This motivates us to
				consider conditions on \(y_1\) and \(y_2\) that imply <a href="#x1-46r25">(5.1.25)</a>.
			</div>

			<div class="theorem" id="002fe4c0-e999-4687-9729-bba038a3cc7f">
				<div>
					<span class="head">Theorem&nbsp;5.1.4</span>
					<a id="x1-484"></a>
					Suppose \(p\) and \(q\) are continuous on \((a,b),\) let \(y_1\) and
					\(y_2\) be solutions of \begin {equation} \label {eq:5.1.27}
					\frac{d^2y}{dx^2}+p(x)\frac{dy}{dx}+q(x)y=0 \end {equation}
					<a id="x1-49r27"></a>
					on \((a,b)\), and define \begin {equation} \label {eq:5.1.28}
					W=y_1\frac{dy_2}{dx}-\frac{dy_1}{dx}y_2\end {equation}
					<a id="x1-50r28"></a>
					Let \(x_0\) be any point in \((a,b).\) Then \begin {equation} \label
					{eq:5.1.29} W(x)=W(x_0) e^{-\int ^x_{x_0}p(t)\, dt}, \quad
					a&lt;x&lt;b\end {equation}
					<a id="x1-51r29"></a>
					Therefore either \(W\) has no zeros in \((a,b)\) or \(W\equiv 0\) on
					\((a,b).\)
				</div>
			</div>

			<div id="5f80fb36-94ba-401c-9379-d9b5b08de13b">
				<span class="ptmb8t-">Proof</span>
				Differentiating <a href="#x1-50r28">(5.1.28)</a> yields \begin {equation} \label {eq:5.1.30}
				\frac{dW}{dx}=\frac{dy_1}{dx}\frac{dy_2}{dx}+y_1\frac{d^2y}{dx^2}_2-\frac{dy_1}{dx}\frac{dy_2}{dx}-\frac{d^2y}{dx^2}_1y_2= y_1\frac{d^2y}{dx^2}_2-\frac{d^2y}{dx^2}_1y_2\end {equation}
				<a id="x1-52r30"></a>
				Since \(y_1\) and \(y_2\) both satisfy <a href="#x1-49r27">(5.1.27)</a>, \[ \frac{d^2y}{dx^2}_1 =-p\frac{dy_1}{dx}-qy_1\quad \mbox{ and } \quad  \frac{d^2y}{dx^2}_2 =-p\frac{dy_2}{dx}-qy_2\]
				Substituting these into <a href="#x1-52r30">(5.1.30)</a> yields
			
				<div class="eqnarray" id="04fb7c9a-0e38-45ed-aafa-b2eff3486bdd">
					\begin {eqnarray*} \frac{dW}{dx}&amp;=&amp; \displaystyle -y_1\bigl (p\frac{dy_2}{dx}+qy_2\bigr
					) +y_2\bigl (p\frac{dy_1}{dx}+qy_1\bigr ) \\ &amp;=&amp; \displaystyle
					-p(y_1\frac{dy_2}{dx}-y_2\frac{dy_1}{dx})-q(y_1y_2-y_2y_1)\\ &amp;=&amp;
					-p(y_1\frac{dy_2}{dx}-y_2\frac{dy_1}{dx})=-pW\end {eqnarray*}
				</div>
			
				Therefore \(\frac{dW}{dx}+p(x)W=0\); that is, \(W\) is the solution of the initial
				value problem \[ \frac{dy}{dx}+p(x)y=0,\quad y(x_0)=W(x_0)\] We leave it to you to
				verify by separation of variables that this implies <a href="#x1-51r29">(5.1.29)</a>. If \(W(x_0)\ne 0\), <a href="#x1-51r29">(5.1.29)</a> implies that \(W\) has no zeros in \((a,b)\), since an exponential is
				never zero. On the other hand, if \(W(x_0)=0\), <a href="#x1-51r29">(5.1.29)</a> implies that \(W(x)=0\) for all \(x\) in \((a,b)\).
			</div>

			<div class="indent" id="cff5628c-49f5-475f-b984-4157b8307aca">
				The function \(W\) defined in <a href="#x1-50r28">(5.1.28)</a> is the
				<span class="vocab">Wronskian</span>
				of \(\{y_1,y_2\}\). Formula <a href="#x1-51r29">(5.1.29)</a> is
				<span class="vocab">Abel’s formula</span>.
			</div>

			<div class="indent" id="25cd85d8-bf67-4ce0-9bc5-850eea3339a8">
				The Wronskian of \(\{y_1,y_2\}\) is usually written as the determinant \[
				W=\left | \begin {array}{cc} y_1 &amp; y_2 \\ \frac{dy_1}{dx} &amp; \frac{dy_2}{dx} \end {array}
				\right |\] The expressions in <a href="#x1-47r26">(5.1.26)</a> for \(c_1\) and \(c_2\) can be written in terms of determinants as \[
				c_1={1\over W(x_0)} \left | \begin {array}{cc} k_0 &amp; y_2(x_0) \\ k_1
				&amp; \frac{dy_2}{dx}(x_0) \end {array} \right | \quad \mbox{ and } \quad  c_2={1\over
				W(x_0)} \left | \begin {array}{cc} y_1(x_0) &amp; k_0 \\ \frac{dy_1}{dx}(x_0)
				&amp;k_1 \end {array} \right |\] If you’ve taken linear algebra you may
				recognize this as <span class="vocab">Cramer’s rule</span>.
			</div>

			<div class="example" id="x1-535" style="--ex-number: 5">
				<div>
					Verify Abel’s formula for the following differential equations and the
					corresponding solutions, from Examples&nbsp;<a href="#x1-61">5.1.1</a>,
					<a href="#x1-142">5.1.2</a>, and
					<a href="#x1-233">5.1.3</a>:
				</div>

				<ol>
					<li>
						\(\frac{d^2y}{dx^2}-y=0;\quad y_1=e^x,\; y_2=e^{-x}\)
					</li>
					<li>
						\(\frac{d^2y}{dx^2}+\omega ^2y=0;\quad \quad y_1=\cos \omega x,\; y_2=\sin \omega x\)
					</li>
					<li>
						\(x^2\frac{d^2y}{dx^2}+x\frac{dy}{dx}-4y=0;\quad y_1=x^2,\; y_2=1/x^2\)
					</li>
				</ol>

				<details class="solution" id="c1a1da63-10d2-4a75-ae84-af58cd259569">
					<summary>Solution</summary>

					<ol>
						<li>
							Since \(p\equiv 0\), we can verify Abel’s formula by showing that \(W\) is
							constant, which is true, since \[ W(x)=\left | \begin {array}{rr} e^x
							&amp; e^{-x} \\ e^x &amp; -e^{-x} \end {array} \right
							|=e^x(-e^{-x})-e^xe^{-x}=-2 \] for all \(x\).
						</li>
						<li>
							Again, since \(p\equiv 0\), we can verify Abel’s formula by showing that
							\(W\) is constant, which is true, since
							<div class="eqnarray" id="aaa5e973-be56-4978-a9f6-a7e6209d480c">
								\begin {eqnarray*} W(x)&amp;=&amp;\displaystyle {\left | \begin
								{array}{cc} \cos \omega x &amp; \sin \omega x \\ -\omega \sin \omega x
								&amp;\omega \cos \omega x \end {array} \right |}\\ &amp;=&amp;\cos \omega
								x (\omega \cos \omega x)-(-\omega \sin \omega x)\sin \omega x\\
								&amp;=&amp;\omega (\cos ^2\omega x+\sin ^2\omega x)=\omega \end
								{eqnarray*}
							</div>
							for all \(x\).
						</li>
						<li>
							Computing the Wronskian of \(y_1=x^2\) and \(y_2=1/x^2\) directly yields
							\begin {equation} \label {eq:5.1.31} W=\left | \begin {array}{cc} x^2
							&amp; 1/x^2 \\ 2x &amp; -2/x^3 \end {array} \right |=x^2\left (-{2\over
							x^3}\right )-2x\left (1\over x^2\right )=-{4\over x}\end {equation}
							<a id="x1-57r31"></a>
							To verify Abel’s formula we rewrite the differential equation as \[
							\frac{d^2y}{dx^2}+{1\over x}\frac{dy}{dx}-{4\over x^2}y=0 \] to see that \(p(x)=1/x\). If \(x_0\)
							and \(x\) are either both in \((-\infty ,0)\) or both in \((0,\infty )\)
							then \[ \int _{x_0}^x p(t)\,dt=\int _{x_0}^x {dt\over t}=\ln \left (x\over
							x_0\right )\] so Abel’s formula becomes
							<div class="eqnarray" id="4ff190d8-3734-49ce-9945-6e51347e427e">
								\begin {eqnarray*} W(x)&amp;=&amp;W(x_0)e^{-\ln (x/x_0)}=W(x_0){x_0\over
								x}\\ &amp;=&amp;-\left (4\over x_0\right )\left (x_0\over x\right ) \quad \mbox
								{from \eqref {eq:5.1.31}}\\ &amp;=&amp;-{4\over x}\end {eqnarray*}
							</div>
							which is consistent with <a href="#x1-57r31">(5.1.31)</a>.
						</li>
					</ol>
				</details>
			</div>

			<div class="indent" id="2cbe302f-9b7a-4c07-8655-953619194ae3">
				The next theorem will enable us to complete the proof of Theorem&nbsp;<a href="#x1-363">5.1.3</a>.
			</div>
			<div class="theorem" id="f4373cd3-83a3-4a23-92b8-3b0d45bc671d">
				<div>
					<span class="head">Theorem&nbsp;5.1.5</span>
					<a id="x1-585"></a>
					Suppose \(p\) and \(q\) are continuous on an open interval \((a,b),\)
					let \(y_1\) and \(y_2\) be solutions of \begin {equation} \label
					{eq:5.1.32} \frac{d^2y}{dx^2}+p(x)\frac{dy}{dx}+q(x)y=0 \end {equation}
					<a id="x1-59r32"></a>
					on \((a,b),\) and let \(W=y_1\frac{dy_2}{dx}-\frac{dy_1}{dx}y_2.\) Then \(y_1\) and \(y_2\)
					are linearly independent on \((a,b)\) if and only if \(W\) has no zeros
					on \((a,b).\)
				</div>
			</div>

			<div id="8086e835-7c83-4ce9-a8e3-22c54f1e8d79">
				<span class="ptmb8t-">Proof</span>
				<div>
					We first show that if \(W(x_0)=0\) for some \(x_0\) in \((a,b)\), then
					\(y_1\) and \(y_2\) are linearly dependent on \((a,b)\). Let \(I\) be a
					subinterval of \((a,b)\) on which \(y_1\) has no zeros. (If there’s no
					such subinterval, \(y_1\equiv 0\) on \((a,b)\), so \(y_1\) and \(y_2\) are
					linearly independent, and we’re finished with this part of the proof.)
					Then \(y_2/y_1\) is defined on \(I\), and \begin {equation} \label
					{eq:5.1.33} \frac{d}{dx} \left (y_2\over y_1\right )={y_1\frac{dy_2}{dx}-\frac{dy_1}{dx}y_2\over
					y_1^2}={W\over y_1^2}\end {equation}
					<a id="x1-60r33"></a>
					However, if \(W(x_0)=0\), Theorem&nbsp;<a href="#x1-484">5.1.4</a>
					implies that \(W\equiv 0\) on \((a,b)\). Therefore <a href="#x1-60r33">(5.1.33)</a> implies that \(\displaystyle \frac{d}{dx} \frac{y_2}{y_1} \equiv 0\), so \(\displaystyle \frac{y_2}{y_1}=c\) (constant) on
					\(I\). This shows that \(y_2(x)=cy_1(x)\) for all \(x\) in \(I\). However,
					we want to show that \(y_2=cy_1(x)\) for all \(x\) in \((a,b)\). Let
					\(Y=y_2-cy_1\). Then \(Y\) is a solution of <a href="#x1-59r32">(5.1.32)</a> on \((a,b)\) such that \(Y\equiv 0\) on \(I\), and therefore \(\frac{dy}{dx}\equiv
					0\) on \(I\). Consequently, if \(x_0\) is chosen arbitrarily in \(I\) then
					\(Y\) is a solution of the initial value problem \[
					\frac{d^2y}{dx^2}+p(x)\frac{dy}{dx}+q(x)y=0,\quad y(x_0)=0,\quad \left.\frac{dy}{dx}\right|_{x_0}=0\] which implies that
					\(Y\equiv 0\) on \((a,b)\), by the paragraph following Theorem&nbsp;<a href="#x1-51">5.1.1</a>.
					(See also Exercise&nbsp;<span class="ptmb8t-">24</span>). Hence, \(y_2-cy_1\equiv 0\) on \((a,b)\), which implies that \(y_1\)
					and \(y_2\) are not linearly independent on \((a,b)\).
				</div>

				<div class="indent" id="c5c252f2-a87e-42a2-a7f3-febd044cbb5f">
					Now suppose \(W\) has no zeros on \((a,b)\). Then \(y_1\) can’t be
					identically zero on \((a,b)\) (why not?), and therefore there is a
					subinterval \(I\) of \((a,b)\) on which \(y_1\) has no zeros. Since <a href="#x1-60r33">(5.1.33)</a> implies that \(y_2/y_1\) is nonconstant on \(I\), \(y_2\) isn’t a
					constant multiple of \(y_1\) on \((a,b)\). A similar argument shows that
					\(y_1\) isn’t a constant multiple of \(y_2\) on \((a,b)\), since \[ \frac{d}{dx} \left
					(y_1\over y_2\right )={\frac{dy_1}{dx}y_2-y_1\frac{dy_2}{dx}\over y_2^2}=-{W\over y_2^2} \] on
					any subinterval of \((a,b)\) where \(y_2\) has no zeros.
				</div>
			</div>

			<div class="indent" id="b30c4d43-b381-4f06-8eb4-64bd5165478c">
				We can now complete the proof of Theorem&nbsp;<a href="#x1-363">5.1.3</a>.
				From Theorem&nbsp;<a href="#x1-585">5.1.5</a>, two solutions \(y_1\) and \(y_2\) of <a href="#x1-59r32">(5.1.32)</a> are linearly independent on \((a,b)\) if and only if \(W\) has no zeros
				on \((a,b)\). From Theorem&nbsp;<a href="#x1-484">5.1.4</a>
				and the motivating comments preceding it, \(\{y_1,y_2\}\) is a fundamental
				set of solutions of <a href="#x1-59r32">(5.1.32)</a> if and only if \(W\) has no zeros on \((a,b)\). Therefore
				\(\{y_1,y_2\}\) is a fundamental set for <a href="#x1-59r32">(5.1.32)</a> on \((a,b)\) if and only if \(\{y_1,y_2\}\) is linearly independent on
				\((a,b)\). _
			</div>
			<div class="indent" id="b64cceb3-c7b2-4c88-9a30-02102c439421">
				The next theorem summarizes the relationships among the concepts discussed
				in this section.
			</div>
			<div class="theorem" id="80b7bc20-8e17-45e7-b6c1-c556422a8596">
				<div>
					<span class="head">Theorem&nbsp;5.1.6</span>
					<a id="x1-616"></a>
					Suppose \(p\) and \(q\) are continuous on an open interval \((a,b)\) and
					let \(y_1\) and \(y_2\) be solutions of \begin {equation} \label
					{eq:5.1.34} \frac{d^2y}{dx^2}+p(x)\frac{dy}{dx}+q(x)y=0 \end {equation}
					<a id="x1-62r34"></a>
					on \((a,b).\) Then the following statements are equivalent\(; \) that
					is\(,\) they are either all true or all false\(.\)
				</div>

				<ol>
					<li>
						The general solution of \(\eqref {eq:5.1.34}\) on \((a,b)\) is
						\(y=c_1y_1+c_2y_2\).
					</li>
					<li>
						\(\{y_1,y_2\}\) is a fundamental set of solutions of \(\eqref
						{eq:5.1.34}\) on \((a,b).\)
					</li>
					<li>
						\(\{y_1,y_2\}\) is linearly independent on \((a,b).\)
					</li>
					<li>
						The Wronskian of \(\{y_1,y_2\}\) is nonzero at some point in
						\((a,b).\)
					</li>
					<li>
						The Wronskian of \(\{y_1,y_2\}\) is nonzero at all points in
						\((a,b).\)
					</li>
				</ol>
			</div>

			<div class="indent" id="70490c73-9675-460b-8646-d6e6a8ffa3f9">
				We can apply this theorem to an equation written as \[
				P_0(x)\frac{d^2y}{dx^2}+P_1(x)\frac{dy}{dx}+P_2(x)y=0 \] on an interval \((a,b)\) where \(P_0\),
				\(P_1\), and \(P_2\) are continuous and \(P_0\) has no zeros.
			</div>

			<div class="theorem" id="a852152e-d82c-4bef-adde-46a42ddd7f8b">
				<div>
					<span class="head">Theorem&nbsp;5.1.7</span>
					<a id="x1-687"></a>
					Suppose \(c\) is in \((a,b)\) and \(\alpha \) and \(\beta \) are real
					numbers, not both zero. Under the assumptions of Theorem&nbsp;<a href="#x1-687">5.1.7</a>,
					suppose \(y_{1}\) and \(y_{2}\) are solutions of <a href="#x1-62r34">(5.1.34)</a> such that \begin {equation} \label {eq:5.1.35} \alpha y_{1}(c)+\beta
					\left.\frac{dy_1}{dx}\right|_c=0\text { and } \alpha y_{2}(c)+\beta \left.\frac{dy_2}{dx}\right|_c=0=0\end {equation}
					<a id="x1-69r35"></a>
					Then \(\{y_{1},y_{2}\}\) isn’t linearly independent on \((a,b).\)
				</div>
			</div>

			<div id="a771876c-07bd-4fe9-a702-07f159c263b9">
				<span class="ptmb8t-">Proof</span>
				Since \(\alpha \) and \(\beta \) are not both zero, <a href="#x1-69r35">(5.1.35)</a> implies that \[ \left |\begin {array}{ccccccc}
				y_{1}(c)&amp; \left.\frac{dy_1}{dx}\right|_c=0 \\y_{2}(c)&amp; \left.\frac{dy_2}{dx}\right|_c=0 \end {array}\right |=0,
				\text {\; so\; \; } \left |\begin {array}{cccccc} y_{1}(c)&amp;y_{2}(c)\\
				\left.\frac{dy_1}{dx}\right|_c=0 &amp; \left.\frac{dy_2}{dx}\right|_c=0 \end {array}\right |=0 \] and Theorem&nbsp;<a href="#x1-616">5.1.6</a>
				implies the stated conclusion.
			</div>
		</details>

		<nav class = "ex">
			<a href = "sec5.1.ex.html">5.1 Exercises</a>
		</nav>

		<nav class = "book">
			<a href="sec4.0.html">Back (Chapter 4)</a>

			<a href="sec5.0.html">Chapter 5</a>

			<a href="sec5.2.book.html">Next (5.2)</a>
		</div>
	</body>
</html>
